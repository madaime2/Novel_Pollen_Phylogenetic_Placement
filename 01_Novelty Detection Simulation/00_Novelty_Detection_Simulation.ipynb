{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/madaime2/Novel_Pollen_Phylogenetic_Placement/blob/main/01_Novelty%20Detection%20Simulation/00_Novelty_Detection_Simulation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROnXVmiWPGai"
      },
      "outputs": [],
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision \n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "\n",
        "absolute_path = os.path.dirname(\"/content/drive/MyDrive/Podocarpus_Final/Podocarpus_Project/\")\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2I5dsuz0YyD1"
      },
      "outputs": [],
      "source": [
        "# BEGIN HERE (NAIVE CLASSIFIER)\n",
        "\n",
        "mean = np.array([0.5, 0.5, 0.5])\n",
        "std = np.array([0.25, 0.25, 0.25])\n",
        " \n",
        "torch.manual_seed(3)\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "       # transforms.RandomResizedCrop(224),\n",
        "         torchvision.transforms.Resize((224,224)),\n",
        " \n",
        "        torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
        "        torchvision.transforms.RandomVerticalFlip(p=0.5),\n",
        "        torchvision.transforms.RandomRotation((-90,90)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "       # transforms.Resize(256),\n",
        "       torchvision.transforms.Resize((224,224)),\n",
        "       # transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "    ]),\n",
        "}\n",
        "\n",
        "data_dir = absolute_path + \"/Stacks_Podocarpus_WO_Oleifolius_Train_Val/\"\n",
        "\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms[x])\n",
        "                  for x in ['train', 'val']}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=10,\n",
        "                                             shuffle=True, num_workers=0)\n",
        "              for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8DSHTQ5ZkD5"
      },
      "source": [
        "# Load C-CNN (trained on entire dataset with the exception of the pseudo-novel taxon\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPMt6M05Zrkw"
      },
      "outputs": [],
      "source": [
        "PATH = absolute_path + \"/models/C-CNN_Podocarpus_Split_04_WO_Oleifolius.pt\"\n",
        "model =  models.resnext101_32x8d(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, len(class_names)); \n",
        "model.load_state_dict(torch.load(PATH));\n",
        "#model.load_state_dict(torch.load(PATH, map_location=\"cpu\"))\n",
        "model.to(device);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypbdNx12cxd5"
      },
      "source": [
        "## Forward-pass known cross-sectional images to extract their classification scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TM0TorW8ZrqH",
        "outputId": "d9d48d95-979e-42bc-b9d0-ae21d3a71fa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(96, 29) float32 (96,) int64\n",
            "Accuracy: 0.90625\n"
          ]
        }
      ],
      "source": [
        "# Forward-Pass Images to the network to get (1) Softmax probabilities and (2) best classification prediction\n",
        "\n",
        "# Average Logits first, then pass the new logit (average) to the softmax function\n",
        "\n",
        "# Each instance is a subfolder containing multiple (N) cross-sectional images (slices belonging to the original image stack)\n",
        "\n",
        "\n",
        "mean = np.array([0.5, 0.5, 0.5])\n",
        "std = np.array([0.25, 0.25, 0.25])\n",
        "\n",
        "\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    torchvision.transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "def predict(model, image_dir):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    patch_paths = glob.glob(os.path.join(image_dir, '*'))\n",
        "    images = []\n",
        "    for path in patch_paths:\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        image = transform(image)\n",
        "        images.append(image)\n",
        "\n",
        "    images = torch.stack(images, 0).to(device)\n",
        "    logits = model(images)\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    logits = torch.tensor(logits)\n",
        "    logits_mean = logits.mean(0)\n",
        "    probs = torch.softmax(logits_mean, 0).detach().cpu().numpy()\n",
        "    # Compute average and get final prediction\n",
        "    # avg_probs = probs.mean(0)\n",
        "    avg_probs = probs\n",
        "    final_pred = avg_probs.argmax()\n",
        "    return final_pred, avg_probs\n",
        "\n",
        "val_dir = absolute_path + \"/Stacks_Podocarpus_WO_Oleifolius_Train_Val/val\"\n",
        "\n",
        "val_class_dirs = glob.glob(os.path.join(val_dir, '*'))\n",
        "model.eval()\n",
        "class_map = {name: idx for idx, name in enumerate(class_names)}\n",
        "results = []\n",
        "all_preds = []\n",
        "labels = []\n",
        "all_image_dirs = []\n",
        "\n",
        "for d in val_class_dirs:\n",
        "    image_dirs = glob.glob(os.path.join(d, '*'))\n",
        "    for image_dir in image_dirs:\n",
        "        class_name = os.path.basename(image_dir).split('.')[0]\n",
        "        label = class_map[class_name]\n",
        "        labels.append(label)\n",
        "        all_image_dirs.append(image_dir)  \n",
        "        class_id, probs = predict(model, image_dir)\n",
        "        all_preds.append(probs)\n",
        "        results.append((image_dir, class_id))\n",
        "        #print(f'Predict {image_dir} as class {class_id}')\n",
        "\n",
        "all_preds = np.stack(all_preds, 0)\n",
        "labels = np.array(labels)\n",
        "print(all_preds.shape, all_preds.dtype, labels.shape, labels.dtype)\n",
        "print('Accuracy: {}'.format((all_preds.argmax(1) == labels).mean()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VW_Lusddc-Wt"
      },
      "outputs": [],
      "source": [
        "# Concatenate Cross-Sectional Image Directories and Softmax Probabilities, and Get SORTED Probability vectors\n",
        "import numpy as np \n",
        "Dirs_and_Scores = {}\n",
        "for i in range (len(all_image_dirs)):\n",
        "  array_dir = all_image_dirs[i]\n",
        "  array_pred = all_preds[i]\n",
        "  Dirs_and_Scores[array_dir] = array_pred\n",
        "  #Dirs_and_Scores.append(Dir_and_Score)\n",
        "  \n",
        "Stack_Probs_Sorted = []\n",
        "for key in sorted(Dirs_and_Scores.keys()) :\n",
        "   #print(key , \" :: \" , Dirs_and_Scores[key])\n",
        "   #Dirs_and_Scores[key]\n",
        "   Stack_Probs_Sorted.append(Dirs_and_Scores[key])\n",
        "\n",
        "#Sort Cross-Sectional Image Labels \n",
        "labels.sort()\n",
        "StackLabels = labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJr81VKS7GfE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8909e14a-0b50-4772-b8bf-c800921a9918"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "# Save scores of known cross-sectional images\n",
        "C_CNN_Scores_Known = torch.tensor(Stack_Probs_Sorted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCekWotOdCYH"
      },
      "source": [
        "## Forward-pass unknown (pseudo-novel) cross-sectional images to extract their classification scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tH0rArgfc-Y8"
      },
      "outputs": [],
      "source": [
        "# Forward-Pass Images to the network to get (1) Softmax probabilities and (2) best classification prediction\n",
        "\n",
        "# Average Logits first, then pass the new logit (average) to the softmax function\n",
        "\n",
        "# Each instance is a subfolder containing multiple (N) cross-sectional images (slices belonging to the original image stack)\n",
        "\n",
        "mean = np.array([0.5, 0.5, 0.5])\n",
        "std = np.array([0.25, 0.25, 0.25])\n",
        "\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    torchvision.transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "def predict(model, image_dir):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    patch_paths = glob.glob(os.path.join(image_dir, '*'))\n",
        "    images = []\n",
        "    for path in patch_paths:\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        image = transform(image)\n",
        "        images.append(image)\n",
        "\n",
        "    images = torch.stack(images, 0).to(device)\n",
        "    logits = model(images)\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    logits = torch.tensor(logits)\n",
        "    logits_mean = logits.mean(0)\n",
        "    probs = torch.softmax(logits_mean, 0).detach().cpu().numpy()\n",
        "    # Compute average and get final prediction\n",
        "    # avg_probs = probs.mean(0)\n",
        "    avg_probs = probs\n",
        "    final_pred = avg_probs.argmax()\n",
        "    return final_pred, avg_probs\n",
        "\n",
        "val_dir = absolute_path + \"/Pseudonovel_Oleifolius/Cross_Sections_Oleifolius/\"\n",
        "\n",
        "val_class_dirs = glob.glob(os.path.join(val_dir, '*'))\n",
        "model.eval()\n",
        "#class_map = {name: idx for idx, name in enumerate(class_names)}\n",
        "results = []\n",
        "all_preds = []\n",
        "labels = []\n",
        "all_image_dirs = []\n",
        "\n",
        "for d in val_class_dirs:\n",
        "    image_dirs = glob.glob(os.path.join(d, '*'))\n",
        "    for image_dir in image_dirs:\n",
        "        class_name = os.path.basename(image_dir).split('.')[0]\n",
        "        #label = class_map[class_name]\n",
        "        labels.append(label)\n",
        "        all_image_dirs.append(image_dir)  \n",
        "        class_id, probs = predict(model, image_dir)\n",
        "        all_preds.append(probs)\n",
        "        results.append((image_dir, class_id))\n",
        "\n",
        "all_preds = np.stack(all_preds, 0)\n",
        "labels = np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eekNsp9Dc-iV"
      },
      "outputs": [],
      "source": [
        "# Concatenate Cross-Sectional Image Directories and Softmax Probabilities, and Get SORTED Probability vectors\n",
        "import numpy as np \n",
        "Dirs_and_Scores = {}\n",
        "for i in range (len(all_image_dirs)):\n",
        "  array_dir = all_image_dirs[i]\n",
        "  array_pred = all_preds[i]\n",
        "  Dirs_and_Scores[array_dir] = array_pred\n",
        "  #Dirs_and_Scores.append(Dir_and_Score)\n",
        "  \n",
        "Stack_Probs_Sorted = []\n",
        "for key in sorted(Dirs_and_Scores.keys()) :\n",
        "   #print(key , \" :: \" , Dirs_and_Scores[key])\n",
        "   #Dirs_and_Scores[key]\n",
        "   Stack_Probs_Sorted.append(Dirs_and_Scores[key])\n",
        "\n",
        "#Sort Cross-Sectional Image Labels \n",
        "labels.sort()\n",
        "StackLabels = labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxlnFnG5c-kv"
      },
      "outputs": [],
      "source": [
        "# Save scores of unknown (pseudo-novel) cross-sectional images\n",
        "C_CNN_Scores_Unknown = torch.tensor(Stack_Probs_Sorted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubwHJP51YzlV"
      },
      "source": [
        "# Load H-CNN (trained on entire dataset with the exception of the pseudo-novel taxon)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlZhoyGUYzlc"
      },
      "outputs": [],
      "source": [
        "PATH = absolute_path + \"/models/H-CNN_Podocarpus_Split_04_WO_Oleifolius.pt\"\n",
        "model =  models.resnext101_32x8d(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, len(class_names)); \n",
        "model.load_state_dict(torch.load(PATH));\n",
        "#model.load_state_dict(torch.load(PATH, map_location=\"cpu\"))\n",
        "model.to(device);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14uhspqiYzld"
      },
      "source": [
        "## Forward-pass known MIP images to extract their classification scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0njnlr6Yzld",
        "outputId": "b6a7b662-f8fe-48b0-f396-08808a90e144"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(96, 29) float32 (96,) int64\n",
            "Accuracy: 0.5416666666666666\n"
          ]
        }
      ],
      "source": [
        "# Forward-Pass Images to the network to get (1) Softmax probabilities and (2) best classification prediction\n",
        "\n",
        "# Average Logits first, then pass the new logit (average) to the softmax function\n",
        "\n",
        "# Each instance is a subfolder containing a maximum intensity projection (MIP) image\n",
        "\n",
        "\n",
        "mean = np.array([0.5, 0.5, 0.5])\n",
        "std = np.array([0.25, 0.25, 0.25])\n",
        "\n",
        "\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    torchvision.transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "def predict(model, image_dir):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    patch_paths = glob.glob(os.path.join(image_dir, '*'))\n",
        "    images = []\n",
        "    for path in patch_paths:\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        image = transform(image)\n",
        "        images.append(image)\n",
        "\n",
        "    images = torch.stack(images, 0).to(device)\n",
        "    logits = model(images)\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    logits = torch.tensor(logits)\n",
        "    logits_mean = logits.mean(0)\n",
        "    probs = torch.softmax(logits_mean, 0).detach().cpu().numpy()\n",
        "    # Compute average and get final prediction\n",
        "    # avg_probs = probs.mean(0)\n",
        "    avg_probs = probs\n",
        "    final_pred = avg_probs.argmax()\n",
        "    return final_pred, avg_probs\n",
        "\n",
        "val_dir = absolute_path + \"/Images_Podocarpus_WO_Oleifolius_Train_Val/val\"\n",
        "\n",
        "val_class_dirs = glob.glob(os.path.join(val_dir, '*'))\n",
        "model.eval()\n",
        "class_map = {name: idx for idx, name in enumerate(class_names)}\n",
        "results = []\n",
        "all_preds = []\n",
        "labels = []\n",
        "all_image_dirs = []\n",
        "\n",
        "for d in val_class_dirs:\n",
        "    image_dirs = glob.glob(os.path.join(d, '*'))\n",
        "    for image_dir in image_dirs:\n",
        "        class_name = os.path.basename(image_dir).split('.')[0]\n",
        "        label = class_map[class_name]\n",
        "        labels.append(label)\n",
        "        all_image_dirs.append(image_dir)  \n",
        "        class_id, probs = predict(model, image_dir)\n",
        "        all_preds.append(probs)\n",
        "        results.append((image_dir, class_id))\n",
        "        #print(f'Predict {image_dir} as class {class_id}')\n",
        "\n",
        "all_preds = np.stack(all_preds, 0)\n",
        "labels = np.array(labels)\n",
        "print(all_preds.shape, all_preds.dtype, labels.shape, labels.dtype)\n",
        "print('Accuracy: {}'.format((all_preds.argmax(1) == labels).mean()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mReKeUEmYzld"
      },
      "outputs": [],
      "source": [
        "# Concatenate Cross-Sectional Image Directories and Softmax Probabilities, and Get SORTED Probability vectors\n",
        "import numpy as np \n",
        "Dirs_and_Scores = {}\n",
        "for i in range (len(all_image_dirs)):\n",
        "  array_dir = all_image_dirs[i]\n",
        "  array_pred = all_preds[i]\n",
        "  Dirs_and_Scores[array_dir] = array_pred\n",
        "  #Dirs_and_Scores.append(Dir_and_Score)\n",
        "  \n",
        "Image_Probs_Sorted = []\n",
        "for key in sorted(Dirs_and_Scores.keys()) :\n",
        "   #print(key , \" :: \" , Dirs_and_Scores[key])\n",
        "   #Dirs_and_Scores[key]\n",
        "   Image_Probs_Sorted.append(Dirs_and_Scores[key])\n",
        "\n",
        "#Sort Cross-Sectional Image Labels \n",
        "labels.sort()\n",
        "ImageLabels = labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uGtwTUAYzld"
      },
      "outputs": [],
      "source": [
        "# Save scores of known cross-sectional images\n",
        "H_CNN_Scores_Known = torch.tensor(Image_Probs_Sorted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "el9ZBhR8Yzle"
      },
      "source": [
        "## Forward-pass unknown (pseudo-novel) MIP images to extract their classification scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7STbpj5Yzle"
      },
      "outputs": [],
      "source": [
        "# Forward-Pass Images to the network to get (1) Softmax probabilities and (2) best classification prediction\n",
        "\n",
        "# Average Logits first, then pass the new logit (average) to the softmax function\n",
        "\n",
        "# Each instance is a subfolder containing a maximum intensity projection (MIP) image\n",
        "\n",
        "mean = np.array([0.5, 0.5, 0.5])\n",
        "std = np.array([0.25, 0.25, 0.25])\n",
        "\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    torchvision.transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "def predict(model, image_dir):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    patch_paths = glob.glob(os.path.join(image_dir, '*'))\n",
        "    images = []\n",
        "    for path in patch_paths:\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        image = transform(image)\n",
        "        images.append(image)\n",
        "\n",
        "    images = torch.stack(images, 0).to(device)\n",
        "    logits = model(images)\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    logits = torch.tensor(logits)\n",
        "    logits_mean = logits.mean(0)\n",
        "    probs = torch.softmax(logits_mean, 0).detach().cpu().numpy()\n",
        "    # Compute average and get final prediction\n",
        "    # avg_probs = probs.mean(0)\n",
        "    avg_probs = probs\n",
        "    final_pred = avg_probs.argmax()\n",
        "    return final_pred, avg_probs\n",
        "\n",
        "val_dir = absolute_path + \"/Pseudonovel_Oleifolius/MIP_Oleifolius/\"\n",
        "\n",
        "val_class_dirs = glob.glob(os.path.join(val_dir, '*'))\n",
        "model.eval()\n",
        "#class_map = {name: idx for idx, name in enumerate(class_names)}\n",
        "results = []\n",
        "all_preds = []\n",
        "labels = []\n",
        "all_image_dirs = []\n",
        "\n",
        "for d in val_class_dirs:\n",
        "    image_dirs = glob.glob(os.path.join(d, '*'))\n",
        "    for image_dir in image_dirs:\n",
        "        class_name = os.path.basename(image_dir).split('.')[0]\n",
        "        #label = class_map[class_name]\n",
        "        labels.append(label)\n",
        "        all_image_dirs.append(image_dir)  \n",
        "        class_id, probs = predict(model, image_dir)\n",
        "        all_preds.append(probs)\n",
        "        results.append((image_dir, class_id))\n",
        "\n",
        "all_preds = np.stack(all_preds, 0)\n",
        "labels = np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JWwWZ_CYzle"
      },
      "outputs": [],
      "source": [
        "# Concatenate Cross-Sectional Image Directories and Softmax Probabilities, and Get SORTED Probability vectors\n",
        "import numpy as np \n",
        "Dirs_and_Scores = {}\n",
        "for i in range (len(all_image_dirs)):\n",
        "  array_dir = all_image_dirs[i]\n",
        "  array_pred = all_preds[i]\n",
        "  Dirs_and_Scores[array_dir] = array_pred\n",
        "  #Dirs_and_Scores.append(Dir_and_Score)\n",
        "  \n",
        "Image_Probs_Sorted = []\n",
        "for key in sorted(Dirs_and_Scores.keys()) :\n",
        "   #print(key , \" :: \" , Dirs_and_Scores[key])\n",
        "   #Dirs_and_Scores[key]\n",
        "   Image_Probs_Sorted.append(Dirs_and_Scores[key])\n",
        "\n",
        "#Sort Cross-Sectional Image Labels \n",
        "labels.sort()\n",
        "ImageLabels = labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGf9jm_HYzle"
      },
      "outputs": [],
      "source": [
        "# Save scores of unknown (pseudo-novel) cross-sectional images\n",
        "H_CNN_Scores_Unknown = torch.tensor(Image_Probs_Sorted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkoRAMAyZdip"
      },
      "source": [
        "# Load P-CNN (trained on entire dataset with the exception of the pseudo-novel taxon\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXyvaf7vZdiq"
      },
      "outputs": [],
      "source": [
        "PATH = absolute_path + \"/models/P-CNN_Podocarpus_Split_04_WO_Oleifolius.pt\"\n",
        "model =  models.resnext101_32x8d(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, len(class_names)); \n",
        "model.load_state_dict(torch.load(PATH));\n",
        "#model.load_state_dict(torch.load(PATH, map_location=\"cpu\"))\n",
        "model.to(device);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-imk-ubDZdiq"
      },
      "source": [
        "## Forward-pass known patches to extract their classification scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBPjhZlYZdiq",
        "outputId": "f244901c-3c71-45d0-96e7-21aedfc50f2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(96, 29) float32 (96,) int64\n",
            "Accuracy: 0.7708333333333334\n"
          ]
        }
      ],
      "source": [
        "# Forward Pass Images to the network to get (1) Softmax probabilities and (2) best classification prediction\n",
        "\n",
        "# Average Logits first, then pass the new logit (average) to the softmax function\n",
        "\n",
        "# Each instance is a subfolder containing multiple (N) patches (patches extracted from *one* image)\n",
        "\n",
        "\n",
        "mean = np.array([0.5, 0.5, 0.5])\n",
        "std = np.array([0.25, 0.25, 0.25])\n",
        "\n",
        "\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    torchvision.transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "def predict(model, image_dir):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    patch_paths = glob.glob(os.path.join(image_dir, '*'))\n",
        "    images = []\n",
        "    for path in patch_paths:\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        image = transform(image)\n",
        "        images.append(image)\n",
        "\n",
        "    images = torch.stack(images, 0).to(device)\n",
        "    logits = model(images)\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    logits = torch.tensor(logits)\n",
        "    logits_mean = logits.mean(0)\n",
        "    probs = torch.softmax(logits_mean, 0).detach().cpu().numpy()\n",
        "    # Compute average and get final prediction\n",
        "    # avg_probs = probs.mean(0)\n",
        "    avg_probs = probs\n",
        "    final_pred = avg_probs.argmax()\n",
        "    return final_pred, avg_probs\n",
        "\n",
        "val_dir = absolute_path + \"/Patches_Podocarpus_WO_Oleifolius_Train_Val/val\"\n",
        "\n",
        "val_class_dirs = glob.glob(os.path.join(val_dir, '*'))\n",
        "model.eval()\n",
        "class_map = {name: idx for idx, name in enumerate(class_names)}\n",
        "results = []\n",
        "all_preds = []\n",
        "labels = []\n",
        "all_image_dirs = []\n",
        "\n",
        "for d in val_class_dirs:\n",
        "    image_dirs = glob.glob(os.path.join(d, '*'))\n",
        "    for image_dir in image_dirs:\n",
        "        class_name = os.path.basename(image_dir).split('.')[0]\n",
        "        label = class_map[class_name]\n",
        "        labels.append(label)\n",
        "        all_image_dirs.append(image_dir)  \n",
        "        class_id, probs = predict(model, image_dir)\n",
        "        all_preds.append(probs)\n",
        "        results.append((image_dir, class_id))\n",
        "        #print(f'Predict {image_dir} as class {class_id}')\n",
        "\n",
        "all_preds = np.stack(all_preds, 0)\n",
        "labels = np.array(labels)\n",
        "print(all_preds.shape, all_preds.dtype, labels.shape, labels.dtype)\n",
        "print('Accuracy: {}'.format((all_preds.argmax(1) == labels).mean()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6KAfNvbZdiq"
      },
      "outputs": [],
      "source": [
        "# Concatenate Cross-Sectional Image Directories and Softmax Probabilities, and Get SORTED Probability vectors\n",
        "import numpy as np \n",
        "Dirs_and_Scores = {}\n",
        "for i in range (len(all_image_dirs)):\n",
        "  array_dir = all_image_dirs[i]\n",
        "  array_pred = all_preds[i]\n",
        "  Dirs_and_Scores[array_dir] = array_pred\n",
        "  #Dirs_and_Scores.append(Dir_and_Score)\n",
        "  \n",
        "Patch_Probs_Sorted = []\n",
        "for key in sorted(Dirs_and_Scores.keys()) :\n",
        "   #print(key , \" :: \" , Dirs_and_Scores[key])\n",
        "   #Dirs_and_Scores[key]\n",
        "   Patch_Probs_Sorted.append(Dirs_and_Scores[key])\n",
        "\n",
        "#Sort Cross-Sectional Image Labels \n",
        "labels.sort()\n",
        "PatchLabels = labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlDhJaJCZdir"
      },
      "outputs": [],
      "source": [
        "# Save scores of known cross-sectional images\n",
        "P_CNN_Scores_Known = torch.tensor(Patch_Probs_Sorted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chiaCS9kZdir"
      },
      "source": [
        "## Forward-pass unknown patches to extract their classification scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkLuRAMcZdir"
      },
      "outputs": [],
      "source": [
        "# Forward Pass Images to the network to get (1) Softmax probabilities and (2) best classification prediction\n",
        "\n",
        "# Average Logits first, then pass the new logit (average) to the softmax function\n",
        "\n",
        "# Each instance is a subfolder containing multiple (N) patches (patches extracted from *one* image)\n",
        "\n",
        "mean = np.array([0.5, 0.5, 0.5])\n",
        "std = np.array([0.25, 0.25, 0.25])\n",
        "\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    torchvision.transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "def predict(model, image_dir):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    patch_paths = glob.glob(os.path.join(image_dir, '*'))\n",
        "    images = []\n",
        "    for path in patch_paths:\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        image = transform(image)\n",
        "        images.append(image)\n",
        "\n",
        "    images = torch.stack(images, 0).to(device)\n",
        "    logits = model(images)\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    logits = torch.tensor(logits)\n",
        "    logits_mean = logits.mean(0)\n",
        "    probs = torch.softmax(logits_mean, 0).detach().cpu().numpy()\n",
        "    # Compute average and get final prediction\n",
        "    # avg_probs = probs.mean(0)\n",
        "    avg_probs = probs\n",
        "    final_pred = avg_probs.argmax()\n",
        "    return final_pred, avg_probs\n",
        "\n",
        "val_dir = absolute_path + \"/Pseudonovel_Oleifolius/Cross_Sections_Oleifolius/\"\n",
        "\n",
        "val_class_dirs = glob.glob(os.path.join(val_dir, '*'))\n",
        "model.eval()\n",
        "#class_map = {name: idx for idx, name in enumerate(class_names)}\n",
        "results = []\n",
        "all_preds = []\n",
        "labels = []\n",
        "all_image_dirs = []\n",
        "\n",
        "for d in val_class_dirs:\n",
        "    image_dirs = glob.glob(os.path.join(d, '*'))\n",
        "    for image_dir in image_dirs:\n",
        "        class_name = os.path.basename(image_dir).split('.')[0]\n",
        "        #label = class_map[class_name]\n",
        "        labels.append(label)\n",
        "        all_image_dirs.append(image_dir)  \n",
        "        class_id, probs = predict(model, image_dir)\n",
        "        all_preds.append(probs)\n",
        "        results.append((image_dir, class_id))\n",
        "\n",
        "all_preds = np.stack(all_preds, 0)\n",
        "labels = np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6T56IOphZdir"
      },
      "outputs": [],
      "source": [
        "# Concatenate Cross-Sectional Image Directories and Softmax Probabilities, and Get SORTED Probability vectors\n",
        "import numpy as np \n",
        "Dirs_and_Scores = {}\n",
        "for i in range (len(all_image_dirs)):\n",
        "  array_dir = all_image_dirs[i]\n",
        "  array_pred = all_preds[i]\n",
        "  Dirs_and_Scores[array_dir] = array_pred\n",
        "  #Dirs_and_Scores.append(Dir_and_Score)\n",
        "  \n",
        "Patch_Probs_Sorted = []\n",
        "for key in sorted(Dirs_and_Scores.keys()) :\n",
        "   #print(key , \" :: \" , Dirs_and_Scores[key])\n",
        "   #Dirs_and_Scores[key]\n",
        "   Patch_Probs_Sorted.append(Dirs_and_Scores[key])\n",
        "\n",
        "#Sort Cross-Sectional Image Labels \n",
        "labels.sort()\n",
        "PatchLabels = labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4-nV2WaZdir"
      },
      "outputs": [],
      "source": [
        "# Save scores of unknown (pseudo-novel) cross-sectional images\n",
        "P_CNN_Scores_Unknown = torch.tensor(Patch_Probs_Sorted)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute fused scores"
      ],
      "metadata": {
        "id": "6nRFcJgukQ0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Fused Classification_Scores (element-wise multiplication) - Known specimens \n",
        "Fused_Scores_Known = C_CNN_Scores_Known * H_CNN_Scores_Known * P_CNN_Scores_Known"
      ],
      "metadata": {
        "id": "_SK1WyFGZcur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Fused Classification_Scores (element-wise multiplication) - unknwon (pseudo-novel) specimens \n",
        "Fused_Scores_Unknown = C_CNN_Scores_Unknown * H_CNN_Scores_Unknown * P_CNN_Scores_Unknown"
      ],
      "metadata": {
        "id": "Lp4aMF4dkaN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize (0-1) fused scores - Known specimens\n",
        "import torch\n",
        "import torch.nn.functional as f\n",
        "Normalized_Prob_Fused_Scores_Known = f.normalize(torch.tensor(Fused_Scores_Known), p=2, dim=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBPHMB5OZc19",
        "outputId": "110c7a5d-b779-4274-bd6b-4b44d1dad65c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  after removing the cwd from sys.path.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize (0-1) fused scores - Unknown specimens\n",
        "import torch\n",
        "import torch.nn.functional as f\n",
        "Normalized_Prob_Fused_Scores_Unknown = f.normalize(torch.tensor(Fused_Scores_Unknown), p=2, dim=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TmFFlTYlLfd",
        "outputId": "3c726a50-010c-47af-cc88-b5b6b4adff7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  after removing the cwd from sys.path.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define Shannon Entropy"
      ],
      "metadata": {
        "id": "bfVzzx2SlY4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shannon Entropy \n",
        "def myEntropy(p, dim = -1, keepdim=False):\n",
        "    return -torch.where(p > 0, p * torch.log(p), p.new([0.0])).sum(dim = dim, keepdim=keepdim) "
      ],
      "metadata": {
        "id": "XWf5OYzAlX3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute entropy across all three modalities for both Known and Unknown specimens"
      ],
      "metadata": {
        "id": "_hCsk0IVllMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Entropy_C_CNN_Known = myEntropy(C_CNN_Scores_Known)\n",
        "Entropy_H_CNN_Known = myEntropy(H_CNN_Scores_Known)\n",
        "Entropy_P_CNN_Known = myEntropy(P_CNN_Scores_Known)\n",
        "Entropy_FM_Known = myEntropy(Normalized_Prob_Fused_Scores_Known)"
      ],
      "metadata": {
        "id": "EwcjcH15lsDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Entropy_C_CNN_Unknown = myEntropy(C_CNN_Scores_Unknown)\n",
        "Entropy_H_CNN_Unknown = myEntropy(H_CNN_Scores_Unknown)\n",
        "Entropy_P_CNN_Unknown = myEntropy(P_CNN_Scores_Unknown)\n",
        "Entropy_FM_Unknown = myEntropy(Normalized_Prob_Fused_Scores_Unknown)"
      ],
      "metadata": {
        "id": "vjirUt0TlsGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDeZayCn1Xw6"
      },
      "source": [
        "Plot the ROC curves - In this example, we generated the ROC curve based on one single model (FM). This model combines classification scores across all three modalities. The curve shows the performance of our model (i.e. its ability to differentiates between known versus unknown pollen specimens) at different entropy thresholds."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_curve, roc_auc_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def evaluate_openset(scores_closeset, scores_openset):    \n",
        "    y_true = np.array([0] * len(scores_closeset) + [1] * len(scores_openset))\n",
        "    y_discriminator = np.concatenate([scores_closeset, scores_openset])\n",
        "    auc_d, roc_to_plot = plot_roc(y_true, y_discriminator, 'Discriminator ROC')\n",
        "    return auc_d, roc_to_plot\n",
        "\n",
        "\n",
        "def plot_roc(y_true, y_score, title=\"Receiver Operating Characteristic\", **options):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
        "    auc_score = roc_auc_score(y_true, y_score)\n",
        "    roc_to_plot = {'tp':tpr, 'fp':fpr, 'thresh':thresholds, 'auc_score':auc_score}\n",
        "    return auc_score, roc_to_plot\n",
        "\n",
        "predConfscores_known = -np.array(Entropy_FM_Known)\n",
        "predConfscores_novel = -np.array(Entropy_FM_Unknown)\n",
        "\n",
        "roc_score, roc_to_plot = evaluate_openset(-predConfscores_known, -predConfscores_novel)\n",
        "\n",
        "plt.figure(figsize=(9,5), dpi=64, facecolor='w', edgecolor='k')\n",
        "ax = plt.gca()\n",
        "plt.plot(roc_to_plot['fp'], roc_to_plot['tp'], linewidth=2)\n",
        "\n",
        "for tick in ax.xaxis.get_major_ticks():\n",
        "    tick.label.set_fontsize(12) \n",
        "for tick in ax.yaxis.get_major_ticks():\n",
        "    tick.label.set_fontsize(12) \n",
        "plt.grid('on')\n",
        "plt.xlabel('False Positive Rate', fontsize=15)\n",
        "plt.ylabel('True Positive Rate', fontsize=15)\n",
        "plt.title('ROC score {:.5f}'.format(roc_score), fontsize=15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "bgY3iITcjdv2",
        "outputId": "98659f1a-f80f-477a-faf2-bc0fb9cabffb"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'ROC Score 0.93939')"
            ]
          },
          "metadata": {},
          "execution_count": 39
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 512x448 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcEAAAGPCAYAAAAk6Wv0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAJ1wAACdcBsW4XtwAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de1hU9do+8HuE5DQcFBMVBkgBD6hQiomaIGImHiCrHUoedphyudtZmm57t69aWNrWspNhpWklmZmK7l5qS69KllQqYmkYSCKgiEogjoAIPL8//LneEJhBZQ3Kuj/XxeWsNd/5rmee3eaetWaxlk5EBERERBrUpqULICIiaikMQSIi0iyGIBERaRZDkIiINIshSEREmsUQJCIizWIIUqsSGhqKtm3bQq/Xw8nJCf7+/njvvffqjTt+/DhiYmLg5uYGOzs7dO3aFfPmzYPRaKw3dv369QgODoajoyPatWuHPn36YPHixbhw4UKDNVRUVGDu3Lm45557oNfr0aFDBwwZMgS7d+9u9vd7q7744gv06NEDdnZ26NmzJ7Zu3WpyfElJCWbMmAF3d3fo9XqMGDECx44dU543Go0YNmwY3Nzc4OTkBIPBgOeeew6VlZXKmMrKSsyfPx/e3t7Q6/UYOHAg0tLS6mxn3LhxcHd3h5OTEzp37oy//vWvKC4uVp6vra3F8uXL4ePjA71ejz59+uDLL79spq6QpghRKxISEiL//Oc/RUSkpqZGNm3aJDqdTvbs2aOMOXLkiDg7O8uUKVMkNzdXqqurJT09XQYMGCD33XefXLp0SRk7Y8YMcXNzk08//VRKSkpEROTYsWPyt7/9Tb799tsGa4iLi5P7779fjh8/LiIiZWVlkpycLGlpaaq858uXL9/U63744QexsbGRL774QqqqquSLL74QW1tb2b9/f6OvGTdunIwcOVLOnTsnFRUVMmvWLPHw8BCj0SgiIlVVVfLzzz8rNRUWFsrQoUNl9uzZyhzPPPOM3HvvvXLy5EmpqqqSFStWiF6vl4KCAmVMRkaGlJeXi4jIH3/8IY8//riMHz9eef71118XT09POXLkiFRXV8vGjRvlrrvukoMHD95UL0i7GILUqvw5BK9xdXWV5cuXK8sjRoyQIUOG1HvtuXPnxNnZWZYuXSoiIt9//70AkG+++eaGaujdu7e89tprJsfk5+fLxIkTxd3dXRwdHSUgIED5BV5RUSHz5s0Tb29vcXFxkSFDhsgPP/ygvHbdunXi7u4u77zzjnh5eYlerxcRkYKCApkwYYJ06dJF7r77bomOjpazZ882WsPUqVMlKiqqzrqoqCh58sknGxxvNBqlTZs2dcK8oqJCrKysZMOGDQ2+prCwUEJDQ2X06NHKuo4dO8rGjRvrjHN3d5clS5Y0OMcff/whEyZMEH9/f2XdgAEDlP+drhk8eLBMmzatwTmIGsPDodRqVVdX49NPP0VxcTF69uwJ4Oqhyl27dmHKlCn1xnfo0AGjR49WDqv9z//8D7p06YLhw4ff0HZDQ0OxfPlyvPbaa9i3bx/Ky8vrPF9RUYGwsDC0bdsWGRkZKC0txaeffgpXV1cAwNy5c5GcnIyUlBQUFRUhKioK4eHhKCgoUOY4c+YMDh8+jCNHjqCoqAiXL1/G8OHD0aVLF2RlZeH333+HtbU1Jk6c2GidGRkZGDBgQJ11QUFBOHToUKOvkasfnOstp6en1xkXExMDBwcHdO7cGYcPH8a8efManePauuvneOGFF+Do6Ij27dsjKSkJixYtMjlHbW1tvTmIzGqx+CVSQUhIiNjY2Iizs7NYWVmJlZWVvPrqq8rzBQUFAkCSk5MbfP28efPE19dXRESmTZsmAwYMuOEaqqqqJCEhQcLDw8XZ2VlsbGxk/Pjxkp+fLyIimzdvlvbt20tlZWW919bU1IidnZ0kJSXVWd+3b19lz2fdunViZWVV57Dtli1bpEuXLlJbW1vvvV7b7vW6du0q7777bp117777rnTr1q3R9zZixAgJDw+XM2fOiNFolKefflp0Ol2De2C1tbWSkZEh8+fPlxMnTijrn3rqKQkICJCcnByprKyUZcuWiU6nk/Dw8Aa3mZ2dLf/85z8lIyNDWffyyy+Lh4eHZGRkSFVVlWzYsEHatGkjPj4+jdZO1BDuCVKr8/zzz6O0tBQlJSWYOnUqdu7cierqagBA+/btYWVlhVOnTjX42oKCAnTs2BEA0LFjxzp7X0111113IS4uDikpKSgpKcHevXtx/PhxPPHEEwCAEydOwNvbGzY2NvVee/78eVRUVKBbt2511vv4+CAvL09Z7tixI+zt7ZXl7OxsFBUVoV27dnBxcYGLiwv8/f1hY2NT53V/5uTkhNLS0jrrSkpK4OTk1Oh727BhA7p06YJ+/frBx8cH7dq1Q48ePdChQ4d6Y3U6HQICAnDvvffikUceUda//vrrCAkJQVhYGDw8PHDixAkMHz68wTmuvfdx48Zh5MiRuHLlCgBg3rx5iI2NxaOPPgo3Nzds374dEyZMaHQOosYwBKnVcnR0xKpVq/D7779j1apVAAA7OzsMGzYMn3zySb3xxcXFSE5OxujRowEAo0ePxunTp7Fr166brkGn0yEoKAjTpk1TDtV5e3sjNzcXVVVV9cZ36NABtra2yMnJqbM+JycHnp6eynKbNnX/r9upUyd4eXmhtLS0zk9lZSUGDRrUYG2BgYHYv39/nXUHDhzAvffe2+j76dixIz766CMUFBSgsLAQTz/9tBJijbly5Qp+++03ZVmv1+PNN99Ebm4uzp07hzfeeANHjx41O0dRUZFyRq61tTUWL16M7Oxs/PHHH/j888/x66+/3vChayIeDqVWpaETY9atWyeurq5SWloqIiI///yzODk5yZNPPil5eXlSXV0thw4dkoEDB0pAQIBypqPI1bNDO3XqJJs2bVJen52dLbNmzWr07NCFCxfKrl27pKysTESunk3ar18/5eSQ8vJy6datm8TGxsq5c+ektrZWjh49Krm5uSIiMnPmTOnbt6/k5OTI5cuX5fXXXxcHBwfJy8tT3o+7u3udbZaVlYnBYJAFCxYodRYVFclnn33WaK/S0tLExsZGtm7dKlVVVbJ161axtbWVn376qdHXHDt2TIqKipQ+hIeHS0REhPL8jz/+KDt37pRLly5JTU2NHDhwQHx9feWxxx5Txpw4cUI5RHvq1CmZOHGi3Hvvvcrh4d9++022bNkiFy5ckNraWjl27JgEBwdLUFCQMseZM2ckOztbamtr5fz58zJ79mwxGAxy/vz5RmsnaghDkFqVhkKwurpa/Pz85IUXXlDWHTt2TKKjo6VDhw5ia2sr3t7eMmfOHLlw4UK9OT/88EMZOHCgODg4iIuLi/Tu3VtefPHFBseKiLzyyivSr18/cXFxEb1eL15eXjJz5kwpLi5Wxpw8eVIef/xx6dSpkzg6OkpgYKCkp6eLyNWQfP7558XT01OcnZ1l8ODBsm/fPuW1DYWgyNXvACdPniwGg0EcHR2lW7duMnPmTJP9+vzzz6V79+5iY2Mj3bt3ly+++KLO87169ZKXX35ZWV67dq24u7uLnZ2deHh4yNy5c6WiokJ5fu/evdK/f39xcnISvV4v3bp1k+eff175QCAikpycLN7e3mJnZycdO3aU6dOnyx9//KE8f+zYMRk8eLA4OzuLg4ODeHl5yfTp06WwsFAZk56eLt27dxcHBwdp166dREdHKx8SiG6EToT3EyQiIm3id4JERKRZDEEiItIshiAREWkWQ5CIiDSLIUhERJpl3dIF3Kzu3bvXu6rGzSgrKzN5hQytY3/MY49MY39MY39Ma67+5OTk1LlowzV3bAh269YNycnJtzxPWloagoODm6Gi1on9MY89Mo39MY39Ma25+hMREdHgeh4OJSIizWIIEhGRZjEEiYhIsxiCRESkWQxBIiLSLIYgERFpFkOQiIg0iyFIRESaxRAkIiLNUj0EZ8yYAXd3d+h0ukbHFBYWIiwsDH5+fggKCkJmZqbaZREREakfgjExMUhPTzc5Zv78+YiMjERWVhYWLFiAuLg4tcsiIiJSPwSHDh0KNzc3k2O2bduG2NhYAMC4ceOQlZWFs2fPql0aERFpXItfQLu4uBh2dnbQ6/UAAJ1OB4PBgPz8fHTs2LGFq9OW1KxzWJmShUuXq5V15RUVsE9LbcGqbn/skWnsj2nsj2nlFRXombUfa6YEqTJ/i4fgjUhMTERiYiKAq7fFSEtLu+U5S0tLm2We1iD+2ws4XlJT/4mLRssXc6dhj0xjf0xjf0wrKFbt93SLh6CrqysqKipw6dIlODg4QESQn58Pg8FQb2xMTAxiYmIAXL0tRnPcXoO3Mfk/urRUAEa0tWoDL1d7AP//U6qdXcsWdptjj0xjf0xjf0wrr6hATw9XBAe34j3BqKgorFmzBrNmzcKOHTvg6+vLQ6EtyMvVHimzQwDwQ0JTsEemsT+msT+mXe2POgEIWODEmKlTp8LDwwMA4OHhgUmTJuH06dMIDAxUxixbtgxJSUnw8/NDfHw8Vq9erXZZRERE6u8Jrl+/vsH1GRkZyuMuXbpg9+7dapdCRERUB68YQ0REmsUQJCIizWIIEhGRZjEEiYhIsxiCRESkWQxBIiLSLIYgERFpFkOQiIg0iyFIRESaxRAkIiLNYggSEZFmMQSJiEizGIJERKRZDEEiItIshiAREWkWQ5CIiDSLIUhERJrFECQiIs1iCBIRkWYxBImISLMYgkREpFkMQSIi0izrli6AzEvNOoeVKVm4dLla1e2cLC5XdX4iotsNQ/AOsDIlCxn5pRbbnoMN/7MgIm3gb7s7wLU9wLZWbeDlaq/qthxsrPHcCD9Vt0FEdLtgCN5BvFztkTI7pKXLICJqNXhiDBERaRZDkIiINIshSEREmsUQJCIizWIIEhGRZjEEiYhIsxiCRESkWQxBIiLSLIYgERFpFkOQiIg0iyFIRESaxRAkIiLNYggSEZFmMQSJiEizGIJERKRZDEEiItIshiAREWkWQ5CIiDSLIUhERJrFECQiIs1iCBIRkWYxBImISLMYgkREpFkMQSIi0iyGIBERaRZDkIiINIshSEREmmWREMzMzERQUBD8/PwQFhaGwsLCemNycnIQEhKCwMBA+Pv7Y82aNZYojYiINMwiIRgXF4cFCxYgKysLkZGRmD9/fr0x//3f/43HH38cGRkZ2LVrF2bPno2LFy9aojwiItIo1UOwqKgI2dnZiIyMBADExsZi27Zt9Qtp0wYXLlwAABiNRrRr1w42NjZql0dERBpmrfYGCgoKYDAYlGW9Xg9bW1sUFxfD1dVVWb9s2TJERERg1apVKC0txeeff462bdvWmSsxMRGJiYkArh4+TUtLu+X6SktLm2UeNZVXVCj/WrrWO6E/LY09Mo39MY39MU3t/qgegk2VkJCAp59+GtOnT8fhw4cxZswY/Prrr3B0dFTGxMTEICYmBgAQERGB4ODgW95uWlpas8yjJvu0VOCiEfZ2dhav9U7oT0tjj0xjf0xjf0xTuz+qh6CHhwfy8/OVZaPRiMrKyjp7gQDw5ptv4uzZswCAgIAAdO7cGZmZmRgwYIDaJRIRkUap/p2gm5sbfHx8sH37dgDA2rVrERUVVW+cl5cXUlJSAAAnT55Ebm4uunXrpnZ5RESkYRY5OzQhIQHx8fHw9fVFUlISli1bBgAIDAzE6dOnAQAffvghXn75ZQQEBGDMmDF455136u0tEhERNSeLfCfo7++PAwcO1FufkZGhPL7//vvx008/WaIcIiIiALxiDBERaRhDkIiINIshSEREmsUQJCIizWIIEhGRZjEEiYhIsxiCRESkWQxBIiLSLIYgERFpFkOQiIg0iyFIRESaxRAkIiLNYggSEZFmMQSJiEizGIJERKRZDEEiItIshiAREWkWQ5CIiDSLIUhERJrFECQiIs1iCBIRkWYxBImISLMYgkREpFkMQSIi0iyGIBERaRZDkIiINIshSEREmsUQJCIizWIIEhGRZjUpBPft24fp06dj7NixAID09HR89913qhZGRESkNrMhuGnTJowaNQoAkJqaCgCora3FwoUL1a2MiIhIZWZDcMmSJfjqq6/w/vvvw8rKCgDQp08fHDlyRPXiiIiI1GQ2BPPz8zFo0CAAgE6nAwC0bdsW1dXV6lZGRESkMrMh6O3tjUOHDtVZd/DgQXTt2lW1ooiIiCzBbAjOnj0bDz/8MD744ANUV1djw4YNiImJwZw5cyxRHxERkWqszQ2YPHkyamtrsXLlSlRXV2PRokWYNWsWJkyYYIn6iIiIVGM2BAFg6tSpmDp1qsqlEBERWZbZw6F9+vRpcH1gYGCzF0NERGRJZkMwNze3wfUnT55s7lqIiIgsqtHDoR9++CEAoKamBuvWrYOIKM/99ttvcHNzU786IiIiFTUagvHx8QCAy5cv46WXXlLWt2nTBp06dcKbb76pfnVEREQqajQET5w4AQCIiIhAcnKyxQoiIiKyFLPfCTIAiYiotWrSn0ikpKRg586dOHv2bJ3vBj/++GPVCiMiIlKb2RB89913MXv2bDz00EP4+uuv8dBDD2Hnzp14+OGHLVHfHSs16xxWpmTh0uVbv8bqyeLyZqiIiIiuZzYE3377bWzbtg2jRo1Cu3btkJSUhM2bN2P37t2WqO+OtTIlCxn5pc06p4NNk3bciYioicz+Vj116pRyP8Frh0LHjx+Pv//973j33XfVre4Odm0PsK1VG3i52t/yfA421nhuhN8tz0NERP/HbAg6OTnh4sWLcHR0hJubG44fPw5XV1eUl/MQXVN4udojZXZIS5dBREQNMHt26KBBg7B161YAwNixYzF27FgMGzYMQ4cOVb04IiIiNZndE/zkk0+Uw6CvvPIKXF1dUVZWhrlz56peHBERkZrMhqCNjY3yuG3btnjhhRcAAN988w3Cw8PVq4yIiEhlJg+HXrp0Cenp6fjjjz+UdYcPH8aDDz6I0aNHq14cERGRmhoNwdTUVLi7u6N///4wGAzYuXMnXnzxRdx///1wd3fHsWPHmryRzMxMBAUFwc/PD2FhYSgsLGxw3Ouvv44ePXqgT58+GDly5I2/GyIiohvQ6OHQBQsW4Mknn0RsbCzee+89TJo0Cd7e3sjIyECPHj1uaCNxcXFYsGABIiMj8eabb2L+/Pn46KOP6ozZvHkzvv76axw6dAh2dnYoKiq6uXdERETURI3uCWZmZmLp0qXw9/fH0qVLce7cOWzZsuWGA7CoqAjZ2dmIjIwEAMTGxmLbtm31xq1cuRJLliyBnZ0dAPBWTUREpLpGQ7Cqqko5KcbBwQHOzs7w8PC44Q0UFBTAYDAoy3q9Hra2tiguLq4zLjMzE7t378bAgQMxcOBAbNmy5Ya3RUREdCMaPRxaU1OD3bt3K38ecf0yAISFhTVbIdXV1Th79izS0tJQUFCAQYMGISAgAD4+PsqYxMREJCYmAgBycnKQlpZ2y9stLS1tlnmuV15RofyrxvyWolZ/WhP2yDT2xzT2xzS1+9NoCFZUVGD48OF11v15WafToaamxuwGPDw8kJ+frywbjUZUVlbC1dW1zjhPT09ER0dDp9PBYDAgODgYGRkZdUIwJiYGMTExAK7e5zA4ONjs9s1JS0trlnmuZ5+WClw0wt7OTpX5LUWt/rQm7JFp7I9p7I9paven0cOhtbW1Jn+aEoDA1e/2fHx8sH37dgDA2rVrERUVVW9cdHQ0du7cCQAoKSnBgQMH4O/vfzPviYiIqEnMXjatOSQkJCA+Ph6+vr5ISkrCsmXLAACBgYE4ffo0AGDOnDk4cuQIevfujaFDh2LBggXo2bOnJcojIiKNssi9efz9/XHgwIF66zMyMpTH9vb22LhxoyXKISIiAmChPUEiIqLbEUOQiIg0iyFIRESa1aQQ3LdvH6ZPn46xY8cCANLT0/Hdd9+pWhgREZHazIbgpk2bMGrUKABXL6oNXP3ziYULF6pbGRERkcrMhuCSJUvw1Vdf4f3334eVlRUAoE+fPjhy5IjqxREREanJbAjm5+dj0KBBAK5eJQa4enPd6upqdSsjIiJSmdkQ9Pb2xqFDh+qsO3jwILp27apaUURERJZgNgRnz56Nhx9+GB988AGqq6uxYcMGxMTEYM6cOZaoj4iISDVmrxgzefJk1NbWYuXKlaiursaiRYswa9YsTJgwwRL1ERERqaZJl02bOnUqpk6dqnIpRERElmU2BKOiovDUU08hIiJCOTGGgNSsc1iZkoVLlxs+QehkcbmFKyIiohtlNgQ9PT0xefJk2NnZYerUqYiNjcU999xjidpuaytTspCRX2p2nIONRa5RTkREN8HsiTFvvfUWTp8+jVdffRX79u2Dr68vwsPDsWnTJkvUd9u6tgfY1qoNfDvqG/wJNLjguRF+LVwpERE1pkm7KTY2Nspd3XNycvDcc89h4sSJePzxx9Wu77bn5WqPlNkhLV0GERHdhCYfqzMajdi4cSPWrFmDjIyMBu8OT0REdCcxG4Lff/891q5di82bN8Pd3R2xsbH497//jY4dO1qiPiIiItWYDcEHH3wQjz32GJKTk/HAAw9YoiYiIiKLMBuChYWFcHJyskQtREREFtVgCObm5sLb2xsAcP78eZw/f77BF/P6oUREdCdrMAT79u2LsrIyAICPj0+9P5IXEeh0OtTU1KhfIRERkUoaDMGjR48qj0+cOGGxYoiIiCypwRA0GAzK44KCAgwePLjemH379sHLy0u9yoiIiFRm9ooxo0aNanD9mDFjmr0YIiIiSzIbgiJSb93ly5d5MW0iIrrjNfonEg888AB0Oh0qKysxdOjQOs/l5eWhf//+qhdHRESkpkZDMDw8HADw448/Yvjw4cr6Nm3aoFOnTrxuKBER3fEaDcFFixYBAHx9fTFx4kSLFURERGQpDYbgtb8DBIDo6GjU1tY2+OI2bcx+pUhERHTbajAEnZ2dlT+Wt7a2bvQkGP6xPBER3ckaDMHk5GTl8a5du3gmKBERtUoNhuCQIUOUx6GhoZaqhYiIyKLMfqm3detWHDt2DACQk5ODBx54AMOGDcPvv/+uenFERERqMhuC//Vf/wUHBwflscFgQNeuXTFr1izViyMiIlJTk+4naDAYICL45ptvkJOTA1tb2zrXFyUiIroTmQ3Btm3bory8HL/++isMBgNcXFxQU1ODy5cvW6I+IiIi1ZgNwfDwcDz++OMoLi5GVFQUAOC3335Dp06dVC+OiIhITWa/E1y9ejUCAgLw0EMP4R//+AeAqyfIPP3006oXR0REpCaze4LOzs5YsmRJnXVjx45VrSAiIiJLMRuCALBjxw6sXr0aeXl58PT0xIwZMxAZGal2bbed1KxzWJmShUuXq3GyuLylyyEioltk9nDoxx9/jJiYGPj5+SEuLg5+fn6YPHkyPvroI0vUd1tZmZKFjPxSZJ81oqrm6vVUHWya9DmCiIhuQ2Z/g69YsQJJSUl1bqc0btw4zJo1C1OmTFG1uNvNpcvVAIC2Vm3g5WoPBxtrPDfCr4WrIiKim2U2BPPy8hAWFlZnXWhoKPLy8lQr6nbn5WqPlNkhLV0GERHdIrOHQw0GA1JTU+us27t3Lzw8PFQrioiIyBLM7gnOmTMHkZGRmDZtGrp164acnBx8+OGHeO211yxRHxERkWrMhuDUqVPh6OiIDz74AF9//TUMBgM++OADPProo5aoj4iISDUmQzAnJwc///wz7rvvPnz99deWqomIiMgiGv1OcMeOHejZsyceeeQR9OzZkyFIREStTqMhuGTJErz00ku4ePEiFi5ciFdeecWSdREREamu0RA8fvw45s6dCwcHBzz//PPIysqyZF1ERESqazQEq6urYWVlBeDq7ZSqqqosVhQREZElNHpizJUrV7Bu3TqICACgqqqqzjIAPPnkk+pXSEREpJJGQ9DNzQ0vvfSSsnz33XfXWdbpdAxBIiK6ozUagrm5uc22kczMTEyePBkXLlyAh4cHEhMT0blz5wbHbtiwAZMmTcLu3bsRGhrabDUQERFdz+xl05pDXFwcFixYgKysLERGRmL+/PkNjjt79iwSEhIwcOBAS5RFREQap3oIFhUVITs7W7n/YGxsLLZt29bg2L///e9YunQpbGxs1C6LiIioaTfVvRUFBQUwGAzKsl6vh62tLYqLi+Hq6qqs37p1K5ydnTF06NBG50pMTERiYiKAq1ezSUtLu+X6SktLmzxPeUWF8m9zbPtOcCP90Sr2yDT2xzT2xzS1+3Nb3BG2pKQES5Yswa5du0yOi4mJQUxMDAAgIiICwcHBt7zttLS0Js9jn5YKXDTC3s6uWbZ9J7iR/mgVe2Qa+2Ma+2Oa2v1R/XCoh4cH8vPzlWWj0YjKyso6e4FHjx5FQUEBAgMD4e3tjR9++AHR0dFISkpSuzwiItKwJoXgxo0bMWLECPTt2xfA1fsJNjWg3Nzc4OPjg+3btwMA1q5di6ioqDpjhgwZgrNnzyI3Nxe5ubkYOHAgPvvss3rjiIiImpPZEHz77bcxf/58DBs2DCdPngQAtG/fHv/617+avJGEhATEx8fD19cXSUlJWLZsGQAgMDAQp0+fvsnSiYiIbo3Z7wTfeecdfPXVV+jVqxeWL18OAOjRo8cNXUvU398fBw4cqLc+IyOjwfF79uxp8txEREQ3y+ye4Llz59CrVy8AV68Sc82fL59GRER0JzIbgn5+fvX2zFJTU9GzZ0+1aiIiIrIIs4dDFy5ciIcffhgzZ85EVVUVlixZgrfffhuffPKJJeojIiJSjdk9wYiICGzduhVHjhyBp6cndu3ahffffx8PPvigJeojIiJSTZP+WH7YsGEYNmyY2rUQERFZlNkQ/P333xt9rmvXrs1ajKWlZp1D/LcXoEtLbdL4k8XlKldERESWZDYEfXx8oNPplLNB/3yGaE1NjXqVWcDKlCwcL6kBYLyh1znY3BZXmyMioltk9rf5iRMn6iyfOnUKL730EiZPnqxaUZZy6XI1AKCtVRt4udo36TUONtZ4boSfmmUREZGFmA1BLy+vessfffQRRo4ciYkTJ6pWmCV5uZ49i9AAABQCSURBVNojZXZIS5dBREQWdlMX0HZxcTH5XSEREdGdwOye4PW3N7p06RLWr1+PPn36qFYUERGRJZgNwfDw8DrLer0e/fv3x5o1a1QrioiIyBLMhmBtba0l6iAiIrI4k98JXrlyBb1790ZlZaWl6iEiIrIYkyF41113oaSkBG3aqH4DeiIiIoszm26xsbHKfQSJiIhak0a/E/z+++8xePBg7N69Gz/++CNWr14NLy+vOnuF3377rUWKJCIiUkOjIThq1CiUlZUhPDy83hmiRERErUGjIXjtWqGLFi2yWDFERESW1Oh3gn++UDYREVFr1OieYHl5OcLCwky++PqryRAREd1JGg1BKysrDB482JK1EBERWVSjIWhjY4P4+HhL1kJERGRR/Ct4IiLSrEZD8NrZoURERK1VoyF48eJFS9ZBRERkcTwcSkREmsUQJCIizWIIEhGRZjEEiYhIsxiCRESkWQxBIiLSLIYgERFpFkOQiIg0iyFIRESaxRAkIiLNYggSEZFmMQSJiEizGIJERKRZDEEiItIshiAREWkWQ5CIiDSLIUhERJrFECQiIs1iCBIRkWYxBImISLMYgkREpFkMQSIi0iyGIBERaRZDkIiINIshSEREmsUQJCIizbJICGZmZiIoKAh+fn4ICwtDYWFhvTEzZ85E9+7dERAQgPDwcJw4ccISpRERkYZZJATj4uKwYMECZGVlITIyEvPnz683ZvTo0Th69CgOHz6MRx55BM8884wlSiMiIg1TPQSLioqQnZ2NyMhIAEBsbCy2bdtWb9zo0aNhbW0NABgwYADy8vLULo2IiDRO9RAsKCiAwWBQlvV6PWxtbVFcXNzoaxISEjBq1Ci1SyMiIo2zbukCrrdq1Sr88ssv2LNnT73nEhMTkZiYCADIyclBWlraLW2rvKJC+fdW52qtSktL2Rsz2CPT2B/T2B/T1O6P6iHo4eGB/Px8ZdloNKKyshKurq71xn766adYvXo19uzZAzs7u3rPx8TEICYmBgAQERGB4ODgW6rNPi0VuGiEvZ3dLc/VWqWlpbE3ZrBHprE/prE/pqndH9UPh7q5ucHHxwfbt28HAKxduxZRUVH1xm3fvh2LFy/Gf/7znwYDkoiIqLlZ5OzQhIQExMfHw9fXF0lJSVi2bBkAIDAwEKdPnwYATJs2DRUVFYiIiEBgYCCGDRtmidKIiEjDLPKdoL+/Pw4cOFBvfUZGhvL43LlzliiFiIhIwSvGEBGRZjEEiYhIsxiCRESkWQxBIiLSLIYgERFpFkOQiIg0iyFIRESaxRAkIiLNYggSEZFmMQSJiEizGIJERKRZDEEiItIshiAREWkWQ5CIiDSLIUhERJrFECQiIs1iCBIRkWYxBImISLMYgkREpFkMQSIi0iyGIBERaRZDkIiINIshSEREmsUQJCIizWIIEhGRZjEEiYhIsxiCRESkWQxBIiLSLIYgERFpFkOQiIg0iyFIRESaxRAkIiLNYggSEZFmMQSJiEizGIJERKRZDEEiItIshiAREWkWQ5CIiDSLIUhERJrFECQiIs1iCBIRkWYxBImISLMYgkREpFkMQSIi0iyGIBERaRZDkIiINIshSEREmsUQJCIizWIIEhGRZjEEiYhIsxiCRESkWQxBIiLSLIYgERFplkVCMDMzE0FBQfDz80NYWBgKCwvrjTEajRg/fjx8fX3Ru3dvfP/995YojYiINMwiIRgXF4cFCxYgKysLkZGRmD9/fr0xy5cvh5eXF7Kzs/Hxxx9jypQpqK2ttUR5RESkUaqHYFFREbKzsxEZGQkAiI2NxbZt2+qN27x5M2bMmAEAuO+++9ChQwccOHBA7fKIiEjDrNXeQEFBAQwGg7Ks1+tha2uL4uJiuLq6Kuvz8/Ph5eWlLHt6eiI/Px8DBgxQ1iUmJiIxMREAkJOTg7S0tFuqrbyiQvn3VudqrUpLS9kbM9gj09gf09gf09Tuj+oh2JxiYmIQExMDAIiIiEBwcPAtzdczaz9QUIyeHq4IDg5qjhJbnbS0tFvuc2vHHpnG/pjG/pimdn9UPxzq4eGB/Px8ZdloNKKysrLOXiAAGAwGnDx5UlnOy8urswephjVTgrAizBlrpjAAiYi0SPUQdHNzg4+PD7Zv3w4AWLt2LaKiouqNe/TRR/Hee+8BANLT03Hu3Dn0799f7fKIiEjDLHJ2aEJCAuLj4+Hr64ukpCQsW7YMABAYGIjTp08DAObOnYsTJ07A19cXkyZNwvr169GmDf+MkYiI1GOR7wT9/f0bPNMzIyNDeezo6IikpCRLlENERASAV4whIiINYwgSEZFmMQSJiEizGIJERKRZDEEiItIshiAREWkWQ5CIiDSLIUhERJrFECQiIs1iCBIRkWbdUbdS+rOcnBxERETc8jynT59Gly5dmqGi1on9MY89Mo39MY39Ma25+pOTk9Pgep2IyC3PfgeLiIhAcnJyS5dx22J/zGOPTGN/TGN/TFO7PzwcSkREmmW1ePHixS1dREvr27dvS5dwW2N/zGOPTGN/TGN/TFOzP5o/HEpERNrFw6FERKRZmgnBzMxMBAUFwc/PD2FhYSgsLKw3xmg0Yvz48fD19UXv3r3x/ffft0ClLaMp/Zk5cya6d++OgIAAhIeH48SJEy1QactoSn+u2bBhA3Q6Hfbs2WO5AltYU/vz+uuvo0ePHujTpw9Gjhxp4SpbTlP6k5OTg5CQEAQGBsLf3x9r1qxpgUpbxowZM+Du7g6dTtfomMLCQoSFhcHPzw9BQUHIzMxsno2LRgwdOlSSkpJEROSNN96QyZMn1xuzcOFCefbZZ0VE5ODBg9KtWzepqamxaJ0tpSn9+fLLL+XKlSsiIvLuu+/KmDFjLFpjS2pKf0REioqKZNCgQTJw4EDZvXu3BStsWU3pz+effy4jRoyQ8vJyERE5c+aMRWtsSU3pz4QJE2TVqlUicrU3jo6OUlZWZtE6W0pqaqqcOXNGTEXS5MmT5Y033hARkaSkJBk6dGizbFsTIXjmzBnp3Lmzsnzx4kVxdHSsN65nz56SmZmpLN9///3y448/WqTGltTU/vzZgQMHpG/fvmqXdlu4kf785S9/kdTUVAkJCdFMCDa1P8HBwZr4/9P1mtqfmJgYeeWVV0RE5Pjx4+Lp6SmXL1+2WJ23A1Mh6OjoKBcvXhQRkdraWunUqZMUFRXd8jY1cTi0oKAABoNBWdbr9bC1tUVxcXGdcfn5+fDy8lKWPT09kZ+fb7E6W0pT+/NnCQkJGDVqlCXKa3FN7c/WrVvh7OyMoUOHWrrEFtXU/mRmZmL37t0YOHAgBg4ciC1btli61BbR1P4sW7YMGzduhIeHBwICApCQkIC2bdtautzbUnFxMezs7KDX6wEAOp0OBoOhWX4/37FXjKGWs2rVKvzyyy+a+s7LnJKSEixZsgS7du1q6VJuW9XV1Th79izS0tJQUFCAQYMGISAgAD4+Pi1d2m0hISEBTz/9NKZPn47Dhw9jzJgx+PXXX+Ho6NjSpbVqmtgT9PDwqPOJwWg0orKyEq6urnXGGQwGnDx5UlnOy8ur8wmutWpqfwDg008/xerVq5GcnAw7OztLltlimtKfo0ePoqCgAIGBgfD29sYPP/yA6OhoJCUltUTJFtXU/348PT0RHR2tfIoPDg5GRkaGpcu1uKb2580338QTTzwBAAgICEDnzp2b7+SPO5yrqysqKipw6dIlAICIID8/v1l+P2siBN3c3ODj44Pt27cDANauXYuoqKh64x599FG89957AID09HScO3cO/fv3t2itLaGp/dm+fTsWL16M//znPw0GZGvVlP4MGTIEZ8+eRW5uLnJzczFw4EB89tlnDfaxtWnqfz/R0dHYuXMngKt7zgcOHIC/v79Fa20JTe2Pl5cXUlJSAAAnT55Ebm4uunXrZtFab2dRUVHKGbM7duyAr68vOnbseOsT3/K3ineII0eOSL9+/cTHx0dCQ0Pl1KlTIiISEBCgPC4rK5PIyEjx8fGRXr16ybffftuSJVtUU/rToUMH8fDwkICAAAkICJDQ0NCWLNmimtKfP9PSiTEiTevPpUuXJDo6Wvz9/aV3796ydu3alizZoprSnx9++EGCgoKkb9++0rt3b9m0aVNLlmxRU6ZMEXd3dwEg7u7u8sQTT8ipU6ckICBAGXPq1CkJDQ0VX19f6devnxw9erRZts0rxhARkWZp4nAoERFRQxiCRESkWQxBIiLSLIYgERFpFkOQiIg0iyFI1IDc3FzodDocP368pUu5IYmJiejevbvJMXv37oVer0dNTY2FqiK6fTEEqVULDQ1F27ZtodfrlZ/b4dqeOp1OuRaiq6srQkJC8N13393yvDExMfjtt9+U5alTpypXIbnmgQcegNFohJWV1S1vryHXPkA4ODhAr9ejQ4cOGDlyJH7++ecbmken0+Gbb75RpUaiaxiC1OrNmzcPRqNR+fn2229buiQAwL///W8YjUbk5+ejT58+GD16NMrKylq6rGZz+PBhGI1G5OTkwMXFBZGRkS1dElE9DEHSpCNHjmD48OG4++674ezsjPvvv9/kxa8PHz6MkJAQuLi4oF27dujXr1+dPa6PP/4YAQEBcHZ2hr+/Pz777LMm12Jvb4+4uDiUlZUhOzsbNTU1WL58Ofz8/ODs7Iz+/fvjq6++Usbn5eUhIiIC7du3h7OzM3r37o29e/cCANavXw8PDw8AwCuvvILExERs2rRJ2QvOy8vDnj17oNPpUF1djaysLFhZWdW5Zi4AjBs3Ds888wwAoKamBq+99hp69uwJZ2dn9OvXD//7v//b5Pfn7OyMSZMmITc3F+fPnwdw9QapY8aMgZubGxwdHdG3b19s3rxZec21y6mNHTsWer1euWPJrdZCVE+zXHeG6DYVEhIi//znP+ut/+WXX2Tnzp1SXl4ulZWVsmjRInFyclLuT3bixAkBINnZ2SIiMmjQIHnxxRflypUrcuXKFTl06JByU9h169aJwWCQ/fv3S01Njezdu1ccHR1l7969jdYFQFJSUkTk6v3lZs6cKS4uLlJWViYrVqwQd3d3OXjwoFy5ckU2btwod911lxw8eFBERCZOnCjTpk2TiooKqampkWPHjsnvv/+u1OLu7q5sZ8qUKRITE1Nn27t37xYAyg2SH3jgAVm0aJHy/KlTp8TKykoOHz4sIiKLFi2SgIAAOXbsmNTU1MjWrVvF3t5ejh8/3uB7u753xcXF8sgjj0inTp2kurpaRETy8/Nly5YtcvHiRamqqpI1a9aItbW1HDlypMEeXXOjtRCZwxCkVi0kJERsbGzE2dlZ+fn4448bHOvs7Cw7duwQkfq/yENDQyU2NrbBX7Z9+vSR1atX11k3bdo0iY2NbbQuAOLg4CAuLi7SqVMnCQ8Pl3379omIiJ+fn3IH7WvGjRsnM2bMEBGRqVOnypgxY+TIkSNSW1tbZ9zNhOBHH30knp6eUlNTIyIiS5YskaCgIGW8k5OTfP3113XmCA8Pl/j4+Abf27XeOTo6iqOjowCQrl27yk8//dRoP0RE+vbtK2+99VadHl0fgjdaC5E5PBxKrd7zzz+P0tJS5WfSpEnIy8tDdHQ0PD094eTkBBcXF5SVleHs2bMNzrF+/XrodDqEhYXBw8MDzz77LIxGIwAgOzsbc+bMgYuLi/KzceNGnD592mRdSUlJKCkpQWFhIVJSUhAcHAzg6s2dr797gI+PD/Ly8gAAK1asgI+PD8aPHw83Nzf89a9/RVFR0U3357HHHsOFCxeQkpICEcGHH36IadOmAQCKiopQVlaGxx57rM7727dvH06dOmVy3vT0dJSVleHo0aMAgF9++UV5rqSkBE899RTuuecepf9Hjx5ttP+3WgtRY3hTXdKkp556Cs7Ozti/fz/c3NwgImjXrh2kkevJe3l54YMPPgAAHD9+HJGRkXBwcMDLL7+MTp064cUXX8TkyZObpTaDwYCcnJw663JycuDp6Qng6r3VVq5ciZUrV+LUqVN44oknMHv2bCQmJtabq00b859z7ezsMHHiRKxZswbW1tYoKirChAkTAAAuLi6wtbXFl19+edNn1fbq1QurV6/Gww8/jIceeghdunTB/PnzcezYMaSmpsJgMECn0yEgIKBO/3U6XZ15mqMWoutxT5A06cKFC9Dr9WjXrh0uXbqEF154Qdmza8j69etRUFAAEYGTkxOsra1hbX31M+Szzz6L+Ph47N+/H7W1tbh8+TL279+PgwcP3lRt06ZNw4oVK5CRkYHq6mp8/vnnSE5OVvbOPvvsM+Tk5KC2thaOjo6wsbFRarlep06dkJOTY/ZvAqdNm4YdO3bgX//6F/7yl78odzO3sbFBXFwc5s2bh8zMTIgIKioq8O233yIrK6vJ72nEiBHo378/Fi1aBOBq/+3t7eHq6oorV67g7bffVvYY/1z7n08+aq5aiP6MIUia9NZbb+Hw4cNo164devXqBXd3d+Wsyobs3r0bAwYMgF6vR0BAAIKDg/GPf/wDADBr1iwsXrwYcXFxaN++Pdzd3TF37lzlLtg3avbs2fjb3/6GRx99FO3bt8err76KrVu3Kjd4Pnz4MMLCwuDo6Ihu3brBxcUFK1asaHCu6dOnAwA6dOgAFxcX5ZDq9e677z74+/tj586dSthes2LFCkyYMEE5DOnt7Y2lS5fiypUrN/S+4uPjsW7dOmRmZmLJkiWoqKiAm5sbvL29UVRUhMGDB9cZv3TpUrz66qtwcXHBmDFjmrUWomt4P0EiItIs7gkSEZFmMQSJiEizGIJERKRZDEEiItIshiAREWkWQ5CIiDSLIUhERJrFECQiIs36f/sQQX26bmueAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eB5A5sJ2mTv",
        "outputId": "ef059e2a-ca25-48c5-9a94-4a48f226b055"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'tp': array([0.        , 0.09090909, 0.36363636, 0.36363636, 0.45454545,\n",
              "        0.45454545, 0.54545455, 0.54545455, 0.63636364, 0.63636364,\n",
              "        0.90909091, 0.90909091, 1.        , 1.        ]),\n",
              " 'fp': array([0.        , 0.        , 0.        , 0.03125   , 0.03125   ,\n",
              "        0.04166667, 0.04166667, 0.0625    , 0.0625    , 0.11458333,\n",
              "        0.11458333, 0.1875    , 0.1875    , 1.        ]),\n",
              " 'thresh': array([3.1006000e+00, 2.1006000e+00, 1.4142737e+00, 1.2290132e+00,\n",
              "        1.1956811e+00, 1.1429478e+00, 1.0422109e+00, 8.8665843e-01,\n",
              "        8.5941952e-01, 5.8310997e-01, 5.5342466e-01, 3.2582155e-01,\n",
              "        2.9276037e-01, 9.6741125e-08], dtype=float32),\n",
              " 'auc_score': 0.9393939393939394}"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "roc_to_plot"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Once the ROC curve is plotted, we can find the optimal entropy threshold that discriminates between known and unknown pollen specimens. This can be achieved by minimizing the index of union (IU) developed by Unal (2017)"
      ],
      "metadata": {
        "id": "5U5KLb5bow6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Solve for lowest AUC value\n",
        "\n",
        "# First, define variables used to minimize the IU.  \n",
        "\n",
        "import numpy as np \n",
        "import torch\n",
        "tpr = roc_to_plot['tp']\n",
        "\n",
        "fpr = roc_to_plot['fp']\n",
        "\n",
        "thresh = roc_to_plot['thresh']\n",
        "\n",
        "Spe = 1 - np.array(fpr)\n",
        "Sen = np.array(tpr)\n",
        "\n",
        "auc_score = roc_to_plot['auc_score']"
      ],
      "metadata": {
        "id": "lKVg62Gn2mYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Solve for lowest AUC value\n",
        "\n",
        "# First, define variables used to minimize the IU.  \n",
        "\n",
        "import numpy as np \n",
        "import torch\n",
        "tpr = np.array([0.        , 0.09090909, 0.36363636, 0.36363636, 0.45454545,\n",
        "        0.45454545, 0.54545455, 0.54545455, 0.63636364, 0.63636364,\n",
        "        0.90909091, 0.90909091, 1.        , 1.        ])\n",
        "\n",
        "fpr = np.array([0.        , 0.        , 0.        , 0.03125   , 0.03125   ,\n",
        "        0.04166667, 0.04166667, 0.0625    , 0.0625    , 0.11458333,\n",
        "        0.11458333, 0.1875    , 0.1875    , 1.        ])\n",
        "\n",
        "thresh = np.array([3.1006000e+00, 2.1006000e+00, 1.4142737e+00, 1.2290132e+00,\n",
        "        1.1956811e+00, 1.1429478e+00, 1.0422109e+00, 8.8665843e-01,\n",
        "        8.5941952e-01, 5.8310997e-01, 5.5342466e-01, 3.2582155e-01,\n",
        "        2.9276037e-01, 9.6741125e-08])\n",
        "\n",
        "Spe = 1 - np.array(fpr)\n",
        "Sen = np.array(tpr)\n",
        "\n",
        "auc_score = 0.9393939393939394"
      ],
      "metadata": {
        "id": "Bf1zunZybYI5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the IU and find the argument that minimizes it\n",
        "\n",
        "IU = torch.abs(torch.tensor((Sen - auc_score))) + torch.abs(torch.tensor((Spe - auc_score)))\n",
        "Diff = torch.abs(torch.tensor(Sen-Spe))\n",
        "IU_minimum_index = np.where(IU == IU.min())\n",
        "Diff_minimum_index = np.where(Diff == Diff.min())\n",
        "\n",
        "if torch.numel(torch.tensor(IU_minimum_index)) == 1:\n",
        "  optimal_threshold = thresh[torch.tensor(IU_minimum_index).item()]\n",
        "else:\n",
        "  optimal_threshold = thresh[torch.tensor(Diff_minimum_index).item()]"
      ],
      "metadata": {
        "id": "pCMTs8bI0Gns",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7ce730c-cd21-4d64-e5c4-dd080ab87a4f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieve optimal Shannon entropy threshold"
      ],
      "metadata": {
        "id": "aWCmltwDkFz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimal threshold \n",
        "print(optimal_threshold)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7cZ-E05C-YN",
        "outputId": "2217c525-55af-45e4-8ab8-f71242e8aca7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.55342466\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Show optimal threshold on ROC curve"
      ],
      "metadata": {
        "id": "nBQvA5qkkFGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_curve, roc_auc_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def evaluate_openset(scores_closeset, scores_openset):    \n",
        "    y_true = np.array([0] * len(scores_closeset) + [1] * len(scores_openset))\n",
        "    y_discriminator = np.concatenate([scores_closeset, scores_openset])\n",
        "    auc_d, roc_to_plot = plot_roc(y_true, y_discriminator, 'Discriminator ROC')\n",
        "    return auc_d, roc_to_plot\n",
        "\n",
        "\n",
        "def plot_roc(y_true, y_score, title=\"Receiver Operating Characteristic\", **options):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
        "    auc_score = roc_auc_score(y_true, y_score)\n",
        "    roc_to_plot = {'tp':tpr, 'fp':fpr, 'thresh':thresholds, 'auc_score':auc_score}\n",
        "    return auc_score, roc_to_plot\n",
        "\n",
        "plt.figure(figsize=(8,7), dpi=64, facecolor='w', edgecolor='k')\n",
        "ax = plt.gca()\n",
        "plt.plot(fpr, tpr, linewidth=3)\n",
        "plt.plot(fpr[10],tpr[10], 'r*', markersize=15)\n",
        "for tick in ax.xaxis.get_major_ticks():\n",
        "    tick.label.set_fontsize(12) \n",
        "for tick in ax.yaxis.get_major_ticks():\n",
        "    tick.label.set_fontsize(12) \n",
        "plt.grid('on')\n",
        "plt.xlabel('False Positive Rate', fontsize=15)\n",
        "plt.ylabel('True Positive Rate', fontsize=15)\n",
        "plt.title('Novelty Detection Simulation', fontsize=15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "osEt4eu4dAdA",
        "outputId": "ebb5b64f-05ce-479e-899a-5e5d3b19cac9"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Novelty Detection Simulation')"
            ]
          },
          "metadata": {},
          "execution_count": 49
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 512x448 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1muKkNnHltTzzzrmp1NaCtvqYgQngC9oC",
      "authorship_tag": "ABX9TyPJLRBm0MpPOGMM7jLV+E4m",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}