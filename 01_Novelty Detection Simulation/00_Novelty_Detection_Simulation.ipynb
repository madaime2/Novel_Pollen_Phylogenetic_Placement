{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/madaime2/Novel_Pollen_Phylogenetic_Placement/blob/main/Novelty%20Detection%20Simulation/01_Novelty_Detection_Simulation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ROnXVmiWPGai"
      },
      "outputs": [],
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision \n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "\n",
        "absolute_path = os.path.dirname(\"/content/drive/MyDrive/Podocarpus_Final/Podocarpus_Project/\")\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2I5dsuz0YyD1"
      },
      "outputs": [],
      "source": [
        "# BEGIN HERE (NAIVE CLASSIFIER)\n",
        "\n",
        "mean = np.array([0.5, 0.5, 0.5])\n",
        "std = np.array([0.25, 0.25, 0.25])\n",
        " \n",
        "torch.manual_seed(3)\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "       # transforms.RandomResizedCrop(224),\n",
        "         torchvision.transforms.Resize((224,224)),\n",
        " \n",
        "        torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
        "        torchvision.transforms.RandomVerticalFlip(p=0.5),\n",
        "        torchvision.transforms.RandomRotation((-90,90)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "       # transforms.Resize(256),\n",
        "       torchvision.transforms.Resize((224,224)),\n",
        "       # transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "    ]),\n",
        "}\n",
        "\n",
        "data_dir = absolute_path + \"/Stacks_Podocarpus_WO_Oleifolius_Train_Val/\"\n",
        "\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms[x])\n",
        "                  for x in ['train', 'val']}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=10,\n",
        "                                             shuffle=True, num_workers=0)\n",
        "              for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8DSHTQ5ZkD5"
      },
      "source": [
        "# Load C-CNN (trained on entire dataset with the exception of the pseudo-novel taxon\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zPMt6M05Zrkw"
      },
      "outputs": [],
      "source": [
        "PATH = absolute_path + \"/models/C-CNN_Podocarpus_Split_04_WO_Oleifolius.pt\"\n",
        "model =  models.resnext101_32x8d(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, len(class_names)); \n",
        "model.load_state_dict(torch.load(PATH));\n",
        "#model.load_state_dict(torch.load(PATH, map_location=\"cpu\"))\n",
        "model.to(device);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypbdNx12cxd5"
      },
      "source": [
        "## Forward-pass known cross-sectional images to extract their classification scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TM0TorW8ZrqH",
        "outputId": "d9d48d95-979e-42bc-b9d0-ae21d3a71fa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(96, 29) float32 (96,) int64\n",
            "Accuracy: 0.90625\n"
          ]
        }
      ],
      "source": [
        "# Forward-Pass Images to the network to get (1) Softmax probabilities and (2) best classification prediction\n",
        "\n",
        "# Average Logits first, then pass the new logit (average) to the softmax function\n",
        "\n",
        "# Each instance is a subfolder containing multiple (N) cross-sectional images (slices belonging to the original image stack)\n",
        "\n",
        "\n",
        "mean = np.array([0.5, 0.5, 0.5])\n",
        "std = np.array([0.25, 0.25, 0.25])\n",
        "\n",
        "\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    torchvision.transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "def predict(model, image_dir):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    patch_paths = glob.glob(os.path.join(image_dir, '*'))\n",
        "    images = []\n",
        "    for path in patch_paths:\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        image = transform(image)\n",
        "        images.append(image)\n",
        "\n",
        "    images = torch.stack(images, 0).to(device)\n",
        "    logits = model(images)\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    logits = torch.tensor(logits)\n",
        "    logits_mean = logits.mean(0)\n",
        "    probs = torch.softmax(logits_mean, 0).detach().cpu().numpy()\n",
        "    # Compute average and get final prediction\n",
        "    # avg_probs = probs.mean(0)\n",
        "    avg_probs = probs\n",
        "    final_pred = avg_probs.argmax()\n",
        "    return final_pred, avg_probs\n",
        "\n",
        "val_dir = absolute_path + \"/Stacks_Podocarpus_WO_Oleifolius_Train_Val/val\"\n",
        "\n",
        "val_class_dirs = glob.glob(os.path.join(val_dir, '*'))\n",
        "model.eval()\n",
        "class_map = {name: idx for idx, name in enumerate(class_names)}\n",
        "results = []\n",
        "all_preds = []\n",
        "labels = []\n",
        "all_image_dirs = []\n",
        "\n",
        "for d in val_class_dirs:\n",
        "    image_dirs = glob.glob(os.path.join(d, '*'))\n",
        "    for image_dir in image_dirs:\n",
        "        class_name = os.path.basename(image_dir).split('.')[0]\n",
        "        label = class_map[class_name]\n",
        "        labels.append(label)\n",
        "        all_image_dirs.append(image_dir)  \n",
        "        class_id, probs = predict(model, image_dir)\n",
        "        all_preds.append(probs)\n",
        "        results.append((image_dir, class_id))\n",
        "        #print(f'Predict {image_dir} as class {class_id}')\n",
        "\n",
        "all_preds = np.stack(all_preds, 0)\n",
        "labels = np.array(labels)\n",
        "print(all_preds.shape, all_preds.dtype, labels.shape, labels.dtype)\n",
        "print('Accuracy: {}'.format((all_preds.argmax(1) == labels).mean()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VW_Lusddc-Wt"
      },
      "outputs": [],
      "source": [
        "# Concatenate Cross-Sectional Image Directories and Softmax Probabilities, and Get SORTED Probability vectors\n",
        "import numpy as np \n",
        "Dirs_and_Scores = {}\n",
        "for i in range (len(all_image_dirs)):\n",
        "  array_dir = all_image_dirs[i]\n",
        "  array_pred = all_preds[i]\n",
        "  Dirs_and_Scores[array_dir] = array_pred\n",
        "  #Dirs_and_Scores.append(Dir_and_Score)\n",
        "  \n",
        "Stack_Probs_Sorted = []\n",
        "for key in sorted(Dirs_and_Scores.keys()) :\n",
        "   #print(key , \" :: \" , Dirs_and_Scores[key])\n",
        "   #Dirs_and_Scores[key]\n",
        "   Stack_Probs_Sorted.append(Dirs_and_Scores[key])\n",
        "\n",
        "#Sort Cross-Sectional Image Labels \n",
        "labels.sort()\n",
        "StackLabels = labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nJr81VKS7GfE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8909e14a-0b50-4772-b8bf-c800921a9918"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "# Save scores of known cross-sectional images\n",
        "C_CNN_Scores_Known = torch.tensor(Stack_Probs_Sorted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCekWotOdCYH"
      },
      "source": [
        "## Forward-pass unknown (pseudo-novel) cross-sectional images to extract their classification scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tH0rArgfc-Y8"
      },
      "outputs": [],
      "source": [
        "# Forward-Pass Images to the network to get (1) Softmax probabilities and (2) best classification prediction\n",
        "\n",
        "# Average Logits first, then pass the new logit (average) to the softmax function\n",
        "\n",
        "# Each instance is a subfolder containing multiple (N) cross-sectional images (slices belonging to the original image stack)\n",
        "\n",
        "mean = np.array([0.5, 0.5, 0.5])\n",
        "std = np.array([0.25, 0.25, 0.25])\n",
        "\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    torchvision.transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "def predict(model, image_dir):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    patch_paths = glob.glob(os.path.join(image_dir, '*'))\n",
        "    images = []\n",
        "    for path in patch_paths:\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        image = transform(image)\n",
        "        images.append(image)\n",
        "\n",
        "    images = torch.stack(images, 0).to(device)\n",
        "    logits = model(images)\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    logits = torch.tensor(logits)\n",
        "    logits_mean = logits.mean(0)\n",
        "    probs = torch.softmax(logits_mean, 0).detach().cpu().numpy()\n",
        "    # Compute average and get final prediction\n",
        "    # avg_probs = probs.mean(0)\n",
        "    avg_probs = probs\n",
        "    final_pred = avg_probs.argmax()\n",
        "    return final_pred, avg_probs\n",
        "\n",
        "val_dir = absolute_path + \"/Pseudonovel_Oleifolius/Cross_Sections_Oleifolius/\"\n",
        "\n",
        "val_class_dirs = glob.glob(os.path.join(val_dir, '*'))\n",
        "model.eval()\n",
        "#class_map = {name: idx for idx, name in enumerate(class_names)}\n",
        "results = []\n",
        "all_preds = []\n",
        "labels = []\n",
        "all_image_dirs = []\n",
        "\n",
        "for d in val_class_dirs:\n",
        "    image_dirs = glob.glob(os.path.join(d, '*'))\n",
        "    for image_dir in image_dirs:\n",
        "        class_name = os.path.basename(image_dir).split('.')[0]\n",
        "        #label = class_map[class_name]\n",
        "        labels.append(label)\n",
        "        all_image_dirs.append(image_dir)  \n",
        "        class_id, probs = predict(model, image_dir)\n",
        "        all_preds.append(probs)\n",
        "        results.append((image_dir, class_id))\n",
        "\n",
        "all_preds = np.stack(all_preds, 0)\n",
        "labels = np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "eekNsp9Dc-iV"
      },
      "outputs": [],
      "source": [
        "# Concatenate Cross-Sectional Image Directories and Softmax Probabilities, and Get SORTED Probability vectors\n",
        "import numpy as np \n",
        "Dirs_and_Scores = {}\n",
        "for i in range (len(all_image_dirs)):\n",
        "  array_dir = all_image_dirs[i]\n",
        "  array_pred = all_preds[i]\n",
        "  Dirs_and_Scores[array_dir] = array_pred\n",
        "  #Dirs_and_Scores.append(Dir_and_Score)\n",
        "  \n",
        "Stack_Probs_Sorted = []\n",
        "for key in sorted(Dirs_and_Scores.keys()) :\n",
        "   #print(key , \" :: \" , Dirs_and_Scores[key])\n",
        "   #Dirs_and_Scores[key]\n",
        "   Stack_Probs_Sorted.append(Dirs_and_Scores[key])\n",
        "\n",
        "#Sort Cross-Sectional Image Labels \n",
        "labels.sort()\n",
        "StackLabels = labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "BxlnFnG5c-kv"
      },
      "outputs": [],
      "source": [
        "# Save scores of unknown (pseudo-novel) cross-sectional images\n",
        "C_CNN_Scores_Unknown = torch.tensor(Stack_Probs_Sorted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubwHJP51YzlV"
      },
      "source": [
        "# Load H-CNN (trained on entire dataset with the exception of the pseudo-novel taxon)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "WlZhoyGUYzlc"
      },
      "outputs": [],
      "source": [
        "PATH = absolute_path + \"/models/H-CNN_Podocarpus_Split_04_WO_Oleifolius.pt\"\n",
        "model =  models.resnext101_32x8d(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, len(class_names)); \n",
        "model.load_state_dict(torch.load(PATH));\n",
        "#model.load_state_dict(torch.load(PATH, map_location=\"cpu\"))\n",
        "model.to(device);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14uhspqiYzld"
      },
      "source": [
        "## Forward-pass known MIP images to extract their classification scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0njnlr6Yzld",
        "outputId": "b6a7b662-f8fe-48b0-f396-08808a90e144"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(96, 29) float32 (96,) int64\n",
            "Accuracy: 0.5416666666666666\n"
          ]
        }
      ],
      "source": [
        "# Forward-Pass Images to the network to get (1) Softmax probabilities and (2) best classification prediction\n",
        "\n",
        "# Average Logits first, then pass the new logit (average) to the softmax function\n",
        "\n",
        "# Each instance is a subfolder containing a maximum intensity projection (MIP) image\n",
        "\n",
        "\n",
        "mean = np.array([0.5, 0.5, 0.5])\n",
        "std = np.array([0.25, 0.25, 0.25])\n",
        "\n",
        "\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    torchvision.transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "def predict(model, image_dir):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    patch_paths = glob.glob(os.path.join(image_dir, '*'))\n",
        "    images = []\n",
        "    for path in patch_paths:\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        image = transform(image)\n",
        "        images.append(image)\n",
        "\n",
        "    images = torch.stack(images, 0).to(device)\n",
        "    logits = model(images)\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    logits = torch.tensor(logits)\n",
        "    logits_mean = logits.mean(0)\n",
        "    probs = torch.softmax(logits_mean, 0).detach().cpu().numpy()\n",
        "    # Compute average and get final prediction\n",
        "    # avg_probs = probs.mean(0)\n",
        "    avg_probs = probs\n",
        "    final_pred = avg_probs.argmax()\n",
        "    return final_pred, avg_probs\n",
        "\n",
        "val_dir = absolute_path + \"/Images_Podocarpus_WO_Oleifolius_Train_Val/val\"\n",
        "\n",
        "val_class_dirs = glob.glob(os.path.join(val_dir, '*'))\n",
        "model.eval()\n",
        "class_map = {name: idx for idx, name in enumerate(class_names)}\n",
        "results = []\n",
        "all_preds = []\n",
        "labels = []\n",
        "all_image_dirs = []\n",
        "\n",
        "for d in val_class_dirs:\n",
        "    image_dirs = glob.glob(os.path.join(d, '*'))\n",
        "    for image_dir in image_dirs:\n",
        "        class_name = os.path.basename(image_dir).split('.')[0]\n",
        "        label = class_map[class_name]\n",
        "        labels.append(label)\n",
        "        all_image_dirs.append(image_dir)  \n",
        "        class_id, probs = predict(model, image_dir)\n",
        "        all_preds.append(probs)\n",
        "        results.append((image_dir, class_id))\n",
        "        #print(f'Predict {image_dir} as class {class_id}')\n",
        "\n",
        "all_preds = np.stack(all_preds, 0)\n",
        "labels = np.array(labels)\n",
        "print(all_preds.shape, all_preds.dtype, labels.shape, labels.dtype)\n",
        "print('Accuracy: {}'.format((all_preds.argmax(1) == labels).mean()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "mReKeUEmYzld"
      },
      "outputs": [],
      "source": [
        "# Concatenate Cross-Sectional Image Directories and Softmax Probabilities, and Get SORTED Probability vectors\n",
        "import numpy as np \n",
        "Dirs_and_Scores = {}\n",
        "for i in range (len(all_image_dirs)):\n",
        "  array_dir = all_image_dirs[i]\n",
        "  array_pred = all_preds[i]\n",
        "  Dirs_and_Scores[array_dir] = array_pred\n",
        "  #Dirs_and_Scores.append(Dir_and_Score)\n",
        "  \n",
        "Image_Probs_Sorted = []\n",
        "for key in sorted(Dirs_and_Scores.keys()) :\n",
        "   #print(key , \" :: \" , Dirs_and_Scores[key])\n",
        "   #Dirs_and_Scores[key]\n",
        "   Image_Probs_Sorted.append(Dirs_and_Scores[key])\n",
        "\n",
        "#Sort Cross-Sectional Image Labels \n",
        "labels.sort()\n",
        "ImageLabels = labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "-uGtwTUAYzld"
      },
      "outputs": [],
      "source": [
        "# Save scores of known cross-sectional images\n",
        "H_CNN_Scores_Known = torch.tensor(Image_Probs_Sorted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "el9ZBhR8Yzle"
      },
      "source": [
        "## Forward-pass unknown (pseudo-novel) MIP images to extract their classification scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "c7STbpj5Yzle"
      },
      "outputs": [],
      "source": [
        "# Forward-Pass Images to the network to get (1) Softmax probabilities and (2) best classification prediction\n",
        "\n",
        "# Average Logits first, then pass the new logit (average) to the softmax function\n",
        "\n",
        "# Each instance is a subfolder containing a maximum intensity projection (MIP) image\n",
        "\n",
        "mean = np.array([0.5, 0.5, 0.5])\n",
        "std = np.array([0.25, 0.25, 0.25])\n",
        "\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    torchvision.transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "def predict(model, image_dir):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    patch_paths = glob.glob(os.path.join(image_dir, '*'))\n",
        "    images = []\n",
        "    for path in patch_paths:\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        image = transform(image)\n",
        "        images.append(image)\n",
        "\n",
        "    images = torch.stack(images, 0).to(device)\n",
        "    logits = model(images)\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    logits = torch.tensor(logits)\n",
        "    logits_mean = logits.mean(0)\n",
        "    probs = torch.softmax(logits_mean, 0).detach().cpu().numpy()\n",
        "    # Compute average and get final prediction\n",
        "    # avg_probs = probs.mean(0)\n",
        "    avg_probs = probs\n",
        "    final_pred = avg_probs.argmax()\n",
        "    return final_pred, avg_probs\n",
        "\n",
        "val_dir = absolute_path + \"/Pseudonovel_Oleifolius/MIP_Oleifolius/\"\n",
        "\n",
        "val_class_dirs = glob.glob(os.path.join(val_dir, '*'))\n",
        "model.eval()\n",
        "#class_map = {name: idx for idx, name in enumerate(class_names)}\n",
        "results = []\n",
        "all_preds = []\n",
        "labels = []\n",
        "all_image_dirs = []\n",
        "\n",
        "for d in val_class_dirs:\n",
        "    image_dirs = glob.glob(os.path.join(d, '*'))\n",
        "    for image_dir in image_dirs:\n",
        "        class_name = os.path.basename(image_dir).split('.')[0]\n",
        "        #label = class_map[class_name]\n",
        "        labels.append(label)\n",
        "        all_image_dirs.append(image_dir)  \n",
        "        class_id, probs = predict(model, image_dir)\n",
        "        all_preds.append(probs)\n",
        "        results.append((image_dir, class_id))\n",
        "\n",
        "all_preds = np.stack(all_preds, 0)\n",
        "labels = np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "0JWwWZ_CYzle"
      },
      "outputs": [],
      "source": [
        "# Concatenate Cross-Sectional Image Directories and Softmax Probabilities, and Get SORTED Probability vectors\n",
        "import numpy as np \n",
        "Dirs_and_Scores = {}\n",
        "for i in range (len(all_image_dirs)):\n",
        "  array_dir = all_image_dirs[i]\n",
        "  array_pred = all_preds[i]\n",
        "  Dirs_and_Scores[array_dir] = array_pred\n",
        "  #Dirs_and_Scores.append(Dir_and_Score)\n",
        "  \n",
        "Image_Probs_Sorted = []\n",
        "for key in sorted(Dirs_and_Scores.keys()) :\n",
        "   #print(key , \" :: \" , Dirs_and_Scores[key])\n",
        "   #Dirs_and_Scores[key]\n",
        "   Image_Probs_Sorted.append(Dirs_and_Scores[key])\n",
        "\n",
        "#Sort Cross-Sectional Image Labels \n",
        "labels.sort()\n",
        "ImageLabels = labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "kGf9jm_HYzle"
      },
      "outputs": [],
      "source": [
        "# Save scores of unknown (pseudo-novel) cross-sectional images\n",
        "H_CNN_Scores_Unknown = torch.tensor(Image_Probs_Sorted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkoRAMAyZdip"
      },
      "source": [
        "# Load P-CNN (trained on entire dataset with the exception of the pseudo-novel taxon\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "fXyvaf7vZdiq"
      },
      "outputs": [],
      "source": [
        "PATH = absolute_path + \"/models/P-CNN_Podocarpus_Split_04_WO_Oleifolius.pt\"\n",
        "model =  models.resnext101_32x8d(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, len(class_names)); \n",
        "model.load_state_dict(torch.load(PATH));\n",
        "#model.load_state_dict(torch.load(PATH, map_location=\"cpu\"))\n",
        "model.to(device);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-imk-ubDZdiq"
      },
      "source": [
        "## Forward-pass known patches to extract their classification scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBPjhZlYZdiq",
        "outputId": "f244901c-3c71-45d0-96e7-21aedfc50f2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(96, 29) float32 (96,) int64\n",
            "Accuracy: 0.7708333333333334\n"
          ]
        }
      ],
      "source": [
        "# Forward Pass Images to the network to get (1) Softmax probabilities and (2) best classification prediction\n",
        "\n",
        "# Average Logits first, then pass the new logit (average) to the softmax function\n",
        "\n",
        "# Each instance is a subfolder containing multiple (N) patches (patches extracted from *one* image)\n",
        "\n",
        "\n",
        "mean = np.array([0.5, 0.5, 0.5])\n",
        "std = np.array([0.25, 0.25, 0.25])\n",
        "\n",
        "\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    torchvision.transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "def predict(model, image_dir):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    patch_paths = glob.glob(os.path.join(image_dir, '*'))\n",
        "    images = []\n",
        "    for path in patch_paths:\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        image = transform(image)\n",
        "        images.append(image)\n",
        "\n",
        "    images = torch.stack(images, 0).to(device)\n",
        "    logits = model(images)\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    logits = torch.tensor(logits)\n",
        "    logits_mean = logits.mean(0)\n",
        "    probs = torch.softmax(logits_mean, 0).detach().cpu().numpy()\n",
        "    # Compute average and get final prediction\n",
        "    # avg_probs = probs.mean(0)\n",
        "    avg_probs = probs\n",
        "    final_pred = avg_probs.argmax()\n",
        "    return final_pred, avg_probs\n",
        "\n",
        "val_dir = absolute_path + \"/Patches_Podocarpus_WO_Oleifolius_Train_Val/val\"\n",
        "\n",
        "val_class_dirs = glob.glob(os.path.join(val_dir, '*'))\n",
        "model.eval()\n",
        "class_map = {name: idx for idx, name in enumerate(class_names)}\n",
        "results = []\n",
        "all_preds = []\n",
        "labels = []\n",
        "all_image_dirs = []\n",
        "\n",
        "for d in val_class_dirs:\n",
        "    image_dirs = glob.glob(os.path.join(d, '*'))\n",
        "    for image_dir in image_dirs:\n",
        "        class_name = os.path.basename(image_dir).split('.')[0]\n",
        "        label = class_map[class_name]\n",
        "        labels.append(label)\n",
        "        all_image_dirs.append(image_dir)  \n",
        "        class_id, probs = predict(model, image_dir)\n",
        "        all_preds.append(probs)\n",
        "        results.append((image_dir, class_id))\n",
        "        #print(f'Predict {image_dir} as class {class_id}')\n",
        "\n",
        "all_preds = np.stack(all_preds, 0)\n",
        "labels = np.array(labels)\n",
        "print(all_preds.shape, all_preds.dtype, labels.shape, labels.dtype)\n",
        "print('Accuracy: {}'.format((all_preds.argmax(1) == labels).mean()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "d6KAfNvbZdiq"
      },
      "outputs": [],
      "source": [
        "# Concatenate Cross-Sectional Image Directories and Softmax Probabilities, and Get SORTED Probability vectors\n",
        "import numpy as np \n",
        "Dirs_and_Scores = {}\n",
        "for i in range (len(all_image_dirs)):\n",
        "  array_dir = all_image_dirs[i]\n",
        "  array_pred = all_preds[i]\n",
        "  Dirs_and_Scores[array_dir] = array_pred\n",
        "  #Dirs_and_Scores.append(Dir_and_Score)\n",
        "  \n",
        "Patch_Probs_Sorted = []\n",
        "for key in sorted(Dirs_and_Scores.keys()) :\n",
        "   #print(key , \" :: \" , Dirs_and_Scores[key])\n",
        "   #Dirs_and_Scores[key]\n",
        "   Patch_Probs_Sorted.append(Dirs_and_Scores[key])\n",
        "\n",
        "#Sort Cross-Sectional Image Labels \n",
        "labels.sort()\n",
        "PatchLabels = labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "AlDhJaJCZdir"
      },
      "outputs": [],
      "source": [
        "# Save scores of known cross-sectional images\n",
        "P_CNN_Scores_Known = torch.tensor(Patch_Probs_Sorted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chiaCS9kZdir"
      },
      "source": [
        "## Forward-pass unknown patches to extract their classification scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "AkLuRAMcZdir"
      },
      "outputs": [],
      "source": [
        "# Forward Pass Images to the network to get (1) Softmax probabilities and (2) best classification prediction\n",
        "\n",
        "# Average Logits first, then pass the new logit (average) to the softmax function\n",
        "\n",
        "# Each instance is a subfolder containing multiple (N) patches (patches extracted from *one* image)\n",
        "\n",
        "mean = np.array([0.5, 0.5, 0.5])\n",
        "std = np.array([0.25, 0.25, 0.25])\n",
        "\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    torchvision.transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "def predict(model, image_dir):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    patch_paths = glob.glob(os.path.join(image_dir, '*'))\n",
        "    images = []\n",
        "    for path in patch_paths:\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        image = transform(image)\n",
        "        images.append(image)\n",
        "\n",
        "    images = torch.stack(images, 0).to(device)\n",
        "    logits = model(images)\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    logits = torch.tensor(logits)\n",
        "    logits_mean = logits.mean(0)\n",
        "    probs = torch.softmax(logits_mean, 0).detach().cpu().numpy()\n",
        "    # Compute average and get final prediction\n",
        "    # avg_probs = probs.mean(0)\n",
        "    avg_probs = probs\n",
        "    final_pred = avg_probs.argmax()\n",
        "    return final_pred, avg_probs\n",
        "\n",
        "val_dir = absolute_path + \"/Pseudonovel_Oleifolius/Cross_Sections_Oleifolius/\"\n",
        "\n",
        "val_class_dirs = glob.glob(os.path.join(val_dir, '*'))\n",
        "model.eval()\n",
        "#class_map = {name: idx for idx, name in enumerate(class_names)}\n",
        "results = []\n",
        "all_preds = []\n",
        "labels = []\n",
        "all_image_dirs = []\n",
        "\n",
        "for d in val_class_dirs:\n",
        "    image_dirs = glob.glob(os.path.join(d, '*'))\n",
        "    for image_dir in image_dirs:\n",
        "        class_name = os.path.basename(image_dir).split('.')[0]\n",
        "        #label = class_map[class_name]\n",
        "        labels.append(label)\n",
        "        all_image_dirs.append(image_dir)  \n",
        "        class_id, probs = predict(model, image_dir)\n",
        "        all_preds.append(probs)\n",
        "        results.append((image_dir, class_id))\n",
        "\n",
        "all_preds = np.stack(all_preds, 0)\n",
        "labels = np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "6T56IOphZdir"
      },
      "outputs": [],
      "source": [
        "# Concatenate Cross-Sectional Image Directories and Softmax Probabilities, and Get SORTED Probability vectors\n",
        "import numpy as np \n",
        "Dirs_and_Scores = {}\n",
        "for i in range (len(all_image_dirs)):\n",
        "  array_dir = all_image_dirs[i]\n",
        "  array_pred = all_preds[i]\n",
        "  Dirs_and_Scores[array_dir] = array_pred\n",
        "  #Dirs_and_Scores.append(Dir_and_Score)\n",
        "  \n",
        "Patch_Probs_Sorted = []\n",
        "for key in sorted(Dirs_and_Scores.keys()) :\n",
        "   #print(key , \" :: \" , Dirs_and_Scores[key])\n",
        "   #Dirs_and_Scores[key]\n",
        "   Patch_Probs_Sorted.append(Dirs_and_Scores[key])\n",
        "\n",
        "#Sort Cross-Sectional Image Labels \n",
        "labels.sort()\n",
        "PatchLabels = labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "T4-nV2WaZdir"
      },
      "outputs": [],
      "source": [
        "# Save scores of unknown (pseudo-novel) cross-sectional images\n",
        "P_CNN_Scores_Unknown = torch.tensor(Patch_Probs_Sorted)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute fused scores"
      ],
      "metadata": {
        "id": "6nRFcJgukQ0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Fused Classification_Scores (element-wise multiplication) - Known specimens \n",
        "Fused_Scores_Known = C_CNN_Scores_Known * H_CNN_Scores_Known * P_CNN_Scores_Known"
      ],
      "metadata": {
        "id": "_SK1WyFGZcur"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Fused Classification_Scores (element-wise multiplication) - unknwon (pseudo-novel) specimens \n",
        "Fused_Scores_Unknown = C_CNN_Scores_Unknown * H_CNN_Scores_Unknown * P_CNN_Scores_Unknown"
      ],
      "metadata": {
        "id": "Lp4aMF4dkaN4"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize (0-1) fused scores - Known specimens\n",
        "import torch\n",
        "import torch.nn.functional as f\n",
        "Normalized_Prob_Fused_Scores_Known = f.normalize(torch.tensor(Fused_Scores_Known), p=2, dim=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBPHMB5OZc19",
        "outputId": "110c7a5d-b779-4274-bd6b-4b44d1dad65c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  after removing the cwd from sys.path.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize (0-1) fused scores - Unknown specimens\n",
        "import torch\n",
        "import torch.nn.functional as f\n",
        "Normalized_Prob_Fused_Scores_Unknown = f.normalize(torch.tensor(Fused_Scores_Unknown), p=2, dim=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TmFFlTYlLfd",
        "outputId": "3c726a50-010c-47af-cc88-b5b6b4adff7b"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  after removing the cwd from sys.path.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define Shannon Entropy"
      ],
      "metadata": {
        "id": "bfVzzx2SlY4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shannon Entropy \n",
        "def myEntropy(p, dim = -1, keepdim=False):\n",
        "    return -torch.where(p > 0, p * torch.log(p), p.new([0.0])).sum(dim = dim, keepdim=keepdim) "
      ],
      "metadata": {
        "id": "XWf5OYzAlX3c"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute entropy across all three modalities for both Known and Unknown specimens"
      ],
      "metadata": {
        "id": "_hCsk0IVllMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Entropy_C_CNN_Known = myEntropy(C_CNN_Scores_Known)\n",
        "Entropy_H_CNN_Known = myEntropy(H_CNN_Scores_Known)\n",
        "Entropy_P_CNN_Known = myEntropy(P_CNN_Scores_Known)\n",
        "Entropy_FM_Known = myEntropy(Normalized_Prob_Fused_Scores_Known)"
      ],
      "metadata": {
        "id": "EwcjcH15lsDa"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Entropy_C_CNN_Unknown = myEntropy(C_CNN_Scores_Unknown)\n",
        "Entropy_H_CNN_Unknown = myEntropy(H_CNN_Scores_Unknown)\n",
        "Entropy_P_CNN_Unknown = myEntropy(P_CNN_Scores_Unknown)\n",
        "Entropy_FM_Unknown = myEntropy(Normalized_Prob_Fused_Scores_Unknown)"
      ],
      "metadata": {
        "id": "vjirUt0TlsGL"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDeZayCn1Xw6"
      },
      "source": [
        "Plot the ROC curves - In this example, we generated the ROC curve based on one single model (FM). This model combines classification scores across all three modalities. The curve shows the performance of our model (i.e. its ability to differentiates between known versus unknown pollen specimens) at different entropy thresholds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "feTP057mzZZN",
        "outputId": "e501192f-83f0-4728-c9dd-fe6deae54c77"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'ROC score 0.93939')"
            ]
          },
          "metadata": {},
          "execution_count": 56
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x320 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_curve, roc_auc_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def evaluate_openset(scores_closeset, scores_openset):    \n",
        "    y_true = np.array([0] * len(scores_closeset) + [1] * len(scores_openset))\n",
        "    y_discriminator = np.concatenate([scores_closeset, scores_openset])\n",
        "    auc_d, roc_to_plot = plot_roc(y_true, y_discriminator, 'Discriminator ROC')\n",
        "    return auc_d, roc_to_plot\n",
        "\n",
        "\n",
        "def plot_roc(y_true, y_score, title=\"Receiver Operating Characteristic\", **options):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
        "    auc_score = roc_auc_score(y_true, y_score)\n",
        "    roc_to_plot = {'tp':tpr, 'fp':fpr, 'thresh':thresholds, 'auc_score':auc_score}\n",
        "    return auc_score, roc_to_plot\n",
        "\n",
        "predConfscores_known = -np.array(Entropy_FM_Known)\n",
        "predConfscores_novel = -np.array(Entropy_FM_Unknown)\n",
        "\n",
        "roc_score, roc_to_plot = evaluate_openset(-predConfscores_known, -predConfscores_novel)\n",
        "\n",
        "plt.figure(figsize=(9,5), dpi=64, facecolor='w', edgecolor='k')\n",
        "ax = plt.gca()\n",
        "plt.plot(roc_to_plot['fp'], roc_to_plot['tp'], linewidth=2)\n",
        "\n",
        "for tick in ax.xaxis.get_major_ticks():\n",
        "    tick.label.set_fontsize(12) \n",
        "for tick in ax.yaxis.get_major_ticks():\n",
        "    tick.label.set_fontsize(12) \n",
        "plt.grid('on')\n",
        "plt.xlabel('False Positive Rate', fontsize=15)\n",
        "plt.ylabel('True Positive Rate', fontsize=15)\n",
        "plt.title('ROC score {:.5f}'.format(roc_score), fontsize=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eB5A5sJ2mTv",
        "outputId": "ef059e2a-ca25-48c5-9a94-4a48f226b055"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'tp': array([0.        , 0.09090909, 0.36363636, 0.36363636, 0.45454545,\n",
              "        0.45454545, 0.54545455, 0.54545455, 0.63636364, 0.63636364,\n",
              "        0.90909091, 0.90909091, 1.        , 1.        ]),\n",
              " 'fp': array([0.        , 0.        , 0.        , 0.03125   , 0.03125   ,\n",
              "        0.04166667, 0.04166667, 0.0625    , 0.0625    , 0.11458333,\n",
              "        0.11458333, 0.1875    , 0.1875    , 1.        ]),\n",
              " 'thresh': array([3.1006000e+00, 2.1006000e+00, 1.4142737e+00, 1.2290132e+00,\n",
              "        1.1956811e+00, 1.1429478e+00, 1.0422109e+00, 8.8665843e-01,\n",
              "        8.5941952e-01, 5.8310997e-01, 5.5342466e-01, 3.2582155e-01,\n",
              "        2.9276037e-01, 9.6741125e-08], dtype=float32),\n",
              " 'auc_score': 0.9393939393939394}"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "roc_to_plot"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Once the ROC curve is plotted, we can find the optimal entropy threshold that discriminates between known and unknown pollen specimens. This can be achieved by minimizing the index of union (IU) developed by Unal (2017)"
      ],
      "metadata": {
        "id": "5U5KLb5bow6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Solve for lowest AUC value\n",
        "\n",
        "# First, define variables used to minimize the IU.  \n",
        "\n",
        "import numpy as np \n",
        "import torch\n",
        "tpr = roc_to_plot['tp']\n",
        "\n",
        "fpr = roc_to_plot['fp']\n",
        "\n",
        "thresh = roc_to_plot['thresh']\n",
        "\n",
        "Spe = 1 - np.array(fpr)\n",
        "Sen = np.array(tpr)\n",
        "\n",
        "auc_score = roc_to_plot['auc_score']"
      ],
      "metadata": {
        "id": "lKVg62Gn2mYq"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the IU and find the argument that minimizes it\n",
        "\n",
        "IU = torch.abs(torch.tensor((Sen - auc_score))) + torch.abs(torch.tensor((Spe - auc_score)))\n",
        "Diff = torch.abs(torch.tensor(Sen-Spe))\n",
        "IU_minimum_index = np.where(IU == IU.min())\n",
        "Diff_minimum_index = np.where(Diff == Diff.min())\n",
        "\n",
        "if torch.numel(torch.tensor(IU_minimum_index)) == 1:\n",
        "  optimal_threshold = thresh[torch.tensor(IU_minimum_index).item()]\n",
        "else:\n",
        "  optimal_threshold = thresh[torch.tensor(Diff_minimum_index).item()]"
      ],
      "metadata": {
        "id": "pCMTs8bI0Gns"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve optimal threshold "
      ],
      "metadata": {
        "id": "u6M35bGYqGeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(optimal_threshold)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7cZ-E05C-YN",
        "outputId": "95971c50-16cf-4bdc-f5cb-91f7a4a67a9d"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.55342466\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1muKkNnHltTzzzrmp1NaCtvqYgQngC9oC",
      "authorship_tag": "ABX9TyPSKrxdEWzuaUNtb/Rf2up1",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
