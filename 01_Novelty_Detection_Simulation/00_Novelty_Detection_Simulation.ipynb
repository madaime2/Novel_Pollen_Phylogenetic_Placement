{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/madaime2/Novel_Pollen_Phylogenetic_Placement/blob/main/01_Novelty_Detection_Simulation/00_Novelty_Detection_Simulation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROnXVmiWPGai"
      },
      "outputs": [],
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision \n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "\n",
        "absolute_path = os.path.dirname(\"/content/drive/MyDrive/Podocarpus_Final/Podocarpus_Project/\")\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2I5dsuz0YyD1"
      },
      "outputs": [],
      "source": [
        "# BEGIN HERE (NAIVE CLASSIFIER)\n",
        "\n",
        "mean = np.array([0.5, 0.5, 0.5])\n",
        "std = np.array([0.25, 0.25, 0.25])\n",
        " \n",
        "torch.manual_seed(3)\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        #transforms.RandomResizedCrop(224),\n",
        "        torchvision.transforms.Resize((224,224)),\n",
        "        torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
        "        torchvision.transforms.RandomVerticalFlip(p=0.5),\n",
        "        torchvision.transforms.RandomRotation((-90,90)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "       # transforms.Resize(256),\n",
        "       torchvision.transforms.Resize((224,224)),\n",
        "       # transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "    ]),\n",
        "}\n",
        "\n",
        "data_dir = absolute_path + \"/Stacks_Podocarpus_WO_Oleifolius_Train_Val/\"\n",
        "\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms[x])\n",
        "                  for x in ['train', 'val']}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=10,\n",
        "                                             shuffle=True, num_workers=0)\n",
        "              for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8DSHTQ5ZkD5"
      },
      "source": [
        "# Load C-CNN (trained on entire dataset with the exception of the pseudo-novel taxon\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPMt6M05Zrkw"
      },
      "outputs": [],
      "source": [
        "PATH = absolute_path + \"/models/C-CNN_Podocarpus_Split_04_WO_Oleifolius.pt\"\n",
        "model =  models.resnext101_32x8d(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, len(class_names)); \n",
        "model.load_state_dict(torch.load(PATH));\n",
        "#model.load_state_dict(torch.load(PATH, map_location=\"cpu\"))\n",
        "model.to(device);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypbdNx12cxd5"
      },
      "source": [
        "## Forward-pass known cross-sectional images to extract their classification scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TM0TorW8ZrqH",
        "outputId": "d9d48d95-979e-42bc-b9d0-ae21d3a71fa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(96, 29) float32 (96,) int64\n",
            "Accuracy: 0.90625\n"
          ]
        }
      ],
      "source": [
        "# Forward-Pass Images to the network to get (1) Softmax probabilities and (2) best classification prediction\n",
        "\n",
        "# Average Logits first, then pass the new logit (average) to the softmax function\n",
        "\n",
        "# Each instance is a subfolder containing multiple (N) cross-sectional images (slices belonging to the original image stack)\n",
        "\n",
        "\n",
        "mean = np.array([0.5, 0.5, 0.5])\n",
        "std = np.array([0.25, 0.25, 0.25])\n",
        "\n",
        "\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    torchvision.transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "def predict(model, image_dir):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    patch_paths = glob.glob(os.path.join(image_dir, '*'))\n",
        "    images = []\n",
        "    for path in patch_paths:\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        image = transform(image)\n",
        "        images.append(image)\n",
        "\n",
        "    images = torch.stack(images, 0).to(device)\n",
        "    logits = model(images)\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    logits = torch.tensor(logits)\n",
        "    logits_mean = logits.mean(0)\n",
        "    probs = torch.softmax(logits_mean, 0).detach().cpu().numpy()\n",
        "    # Compute average and get final prediction\n",
        "    # avg_probs = probs.mean(0)\n",
        "    avg_probs = probs\n",
        "    final_pred = avg_probs.argmax()\n",
        "    return final_pred, avg_probs\n",
        "\n",
        "val_dir = absolute_path + \"/Stacks_Podocarpus_WO_Oleifolius_Train_Val/val\"\n",
        "\n",
        "val_class_dirs = glob.glob(os.path.join(val_dir, '*'))\n",
        "model.eval()\n",
        "class_map = {name: idx for idx, name in enumerate(class_names)}\n",
        "results = []\n",
        "all_preds = []\n",
        "labels = []\n",
        "all_image_dirs = []\n",
        "\n",
        "for d in val_class_dirs:\n",
        "    image_dirs = glob.glob(os.path.join(d, '*'))\n",
        "    for image_dir in image_dirs:\n",
        "        class_name = os.path.basename(image_dir).split('.')[0]\n",
        "        label = class_map[class_name]\n",
        "        labels.append(label)\n",
        "        all_image_dirs.append(image_dir)  \n",
        "        class_id, probs = predict(model, image_dir)\n",
        "        all_preds.append(probs)\n",
        "        results.append((image_dir, class_id))\n",
        "        #print(f'Predict {image_dir} as class {class_id}')\n",
        "\n",
        "all_preds = np.stack(all_preds, 0)\n",
        "labels = np.array(labels)\n",
        "print(all_preds.shape, all_preds.dtype, labels.shape, labels.dtype)\n",
        "print('Accuracy: {}'.format((all_preds.argmax(1) == labels).mean()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VW_Lusddc-Wt"
      },
      "outputs": [],
      "source": [
        "# Concatenate Cross-Sectional Image Directories and Softmax Probabilities, and Get SORTED Probability vectors\n",
        "import numpy as np \n",
        "Dirs_and_Scores = {}\n",
        "for i in range (len(all_image_dirs)):\n",
        "  array_dir = all_image_dirs[i]\n",
        "  array_pred = all_preds[i]\n",
        "  Dirs_and_Scores[array_dir] = array_pred\n",
        "  #Dirs_and_Scores.append(Dir_and_Score)\n",
        "  \n",
        "Stack_Probs_Sorted = []\n",
        "for key in sorted(Dirs_and_Scores.keys()) :\n",
        "   #print(key , \" :: \" , Dirs_and_Scores[key])\n",
        "   #Dirs_and_Scores[key]\n",
        "   Stack_Probs_Sorted.append(Dirs_and_Scores[key])\n",
        "\n",
        "#Sort Cross-Sectional Image Labels \n",
        "labels.sort()\n",
        "StackLabels = labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJr81VKS7GfE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8909e14a-0b50-4772-b8bf-c800921a9918"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "# Save scores of known cross-sectional images\n",
        "C_CNN_Scores_Known = torch.tensor(Stack_Probs_Sorted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCekWotOdCYH"
      },
      "source": [
        "## Forward-pass unknown (pseudo-novel) cross-sectional images to extract their classification scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tH0rArgfc-Y8"
      },
      "outputs": [],
      "source": [
        "# Forward-Pass Images to the network to get (1) Softmax probabilities and (2) best classification prediction\n",
        "\n",
        "# Average Logits first, then pass the new logit (average) to the softmax function\n",
        "\n",
        "# Each instance is a subfolder containing multiple (N) cross-sectional images (slices belonging to the original image stack)\n",
        "\n",
        "mean = np.array([0.5, 0.5, 0.5])\n",
        "std = np.array([0.25, 0.25, 0.25])\n",
        "\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    torchvision.transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "def predict(model, image_dir):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    patch_paths = glob.glob(os.path.join(image_dir, '*'))\n",
        "    images = []\n",
        "    for path in patch_paths:\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        image = transform(image)\n",
        "        images.append(image)\n",
        "\n",
        "    images = torch.stack(images, 0).to(device)\n",
        "    logits = model(images)\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    logits = torch.tensor(logits)\n",
        "    logits_mean = logits.mean(0)\n",
        "    probs = torch.softmax(logits_mean, 0).detach().cpu().numpy()\n",
        "    # Compute average and get final prediction\n",
        "    # avg_probs = probs.mean(0)\n",
        "    avg_probs = probs\n",
        "    final_pred = avg_probs.argmax()\n",
        "    return final_pred, avg_probs\n",
        "\n",
        "val_dir = absolute_path + \"/Pseudonovel_Oleifolius/Cross_Sections_Oleifolius/\"\n",
        "\n",
        "val_class_dirs = glob.glob(os.path.join(val_dir, '*'))\n",
        "model.eval()\n",
        "#class_map = {name: idx for idx, name in enumerate(class_names)}\n",
        "results = []\n",
        "all_preds = []\n",
        "labels = []\n",
        "all_image_dirs = []\n",
        "\n",
        "for d in val_class_dirs:\n",
        "    image_dirs = glob.glob(os.path.join(d, '*'))\n",
        "    for image_dir in image_dirs:\n",
        "        class_name = os.path.basename(image_dir).split('.')[0]\n",
        "        #label = class_map[class_name]\n",
        "        labels.append(label)\n",
        "        all_image_dirs.append(image_dir)  \n",
        "        class_id, probs = predict(model, image_dir)\n",
        "        all_preds.append(probs)\n",
        "        results.append((image_dir, class_id))\n",
        "\n",
        "all_preds = np.stack(all_preds, 0)\n",
        "labels = np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eekNsp9Dc-iV"
      },
      "outputs": [],
      "source": [
        "# Concatenate Cross-Sectional Image Directories and Softmax Probabilities, and Get SORTED Probability vectors\n",
        "import numpy as np \n",
        "Dirs_and_Scores = {}\n",
        "for i in range (len(all_image_dirs)):\n",
        "  array_dir = all_image_dirs[i]\n",
        "  array_pred = all_preds[i]\n",
        "  Dirs_and_Scores[array_dir] = array_pred\n",
        "  #Dirs_and_Scores.append(Dir_and_Score)\n",
        "  \n",
        "Stack_Probs_Sorted = []\n",
        "for key in sorted(Dirs_and_Scores.keys()) :\n",
        "   #print(key , \" :: \" , Dirs_and_Scores[key])\n",
        "   #Dirs_and_Scores[key]\n",
        "   Stack_Probs_Sorted.append(Dirs_and_Scores[key])\n",
        "\n",
        "#Sort Cross-Sectional Image Labels \n",
        "labels.sort()\n",
        "StackLabels = labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxlnFnG5c-kv"
      },
      "outputs": [],
      "source": [
        "# Save scores of unknown (pseudo-novel) cross-sectional images\n",
        "C_CNN_Scores_Unknown = torch.tensor(Stack_Probs_Sorted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubwHJP51YzlV"
      },
      "source": [
        "# Load H-CNN (trained on entire dataset with the exception of the pseudo-novel taxon)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WlZhoyGUYzlc"
      },
      "outputs": [],
      "source": [
        "PATH = absolute_path + \"/models/H-CNN_Podocarpus_Split_04_WO_Oleifolius.pt\"\n",
        "model =  models.resnext101_32x8d(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, len(class_names)); \n",
        "model.load_state_dict(torch.load(PATH));\n",
        "#model.load_state_dict(torch.load(PATH, map_location=\"cpu\"))\n",
        "model.to(device);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14uhspqiYzld"
      },
      "source": [
        "## Forward-pass known MIP images to extract their classification scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0njnlr6Yzld",
        "outputId": "b6a7b662-f8fe-48b0-f396-08808a90e144"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(96, 29) float32 (96,) int64\n",
            "Accuracy: 0.5416666666666666\n"
          ]
        }
      ],
      "source": [
        "# Forward-Pass Images to the network to get (1) Softmax probabilities and (2) best classification prediction\n",
        "\n",
        "# Average Logits first, then pass the new logit (average) to the softmax function\n",
        "\n",
        "# Each instance is a subfolder containing a maximum intensity projection (MIP) image\n",
        "\n",
        "\n",
        "mean = np.array([0.5, 0.5, 0.5])\n",
        "std = np.array([0.25, 0.25, 0.25])\n",
        "\n",
        "\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    torchvision.transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "def predict(model, image_dir):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    patch_paths = glob.glob(os.path.join(image_dir, '*'))\n",
        "    images = []\n",
        "    for path in patch_paths:\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        image = transform(image)\n",
        "        images.append(image)\n",
        "\n",
        "    images = torch.stack(images, 0).to(device)\n",
        "    logits = model(images)\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    logits = torch.tensor(logits)\n",
        "    logits_mean = logits.mean(0)\n",
        "    probs = torch.softmax(logits_mean, 0).detach().cpu().numpy()\n",
        "    # Compute average and get final prediction\n",
        "    # avg_probs = probs.mean(0)\n",
        "    avg_probs = probs\n",
        "    final_pred = avg_probs.argmax()\n",
        "    return final_pred, avg_probs\n",
        "\n",
        "val_dir = absolute_path + \"/Images_Podocarpus_WO_Oleifolius_Train_Val/val\"\n",
        "\n",
        "val_class_dirs = glob.glob(os.path.join(val_dir, '*'))\n",
        "model.eval()\n",
        "class_map = {name: idx for idx, name in enumerate(class_names)}\n",
        "results = []\n",
        "all_preds = []\n",
        "labels = []\n",
        "all_image_dirs = []\n",
        "\n",
        "for d in val_class_dirs:\n",
        "    image_dirs = glob.glob(os.path.join(d, '*'))\n",
        "    for image_dir in image_dirs:\n",
        "        class_name = os.path.basename(image_dir).split('.')[0]\n",
        "        label = class_map[class_name]\n",
        "        labels.append(label)\n",
        "        all_image_dirs.append(image_dir)  \n",
        "        class_id, probs = predict(model, image_dir)\n",
        "        all_preds.append(probs)\n",
        "        results.append((image_dir, class_id))\n",
        "        #print(f'Predict {image_dir} as class {class_id}')\n",
        "\n",
        "all_preds = np.stack(all_preds, 0)\n",
        "labels = np.array(labels)\n",
        "print(all_preds.shape, all_preds.dtype, labels.shape, labels.dtype)\n",
        "print('Accuracy: {}'.format((all_preds.argmax(1) == labels).mean()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mReKeUEmYzld"
      },
      "outputs": [],
      "source": [
        "# Concatenate Cross-Sectional Image Directories and Softmax Probabilities, and Get SORTED Probability vectors\n",
        "import numpy as np \n",
        "Dirs_and_Scores = {}\n",
        "for i in range (len(all_image_dirs)):\n",
        "  array_dir = all_image_dirs[i]\n",
        "  array_pred = all_preds[i]\n",
        "  Dirs_and_Scores[array_dir] = array_pred\n",
        "  #Dirs_and_Scores.append(Dir_and_Score)\n",
        "  \n",
        "Image_Probs_Sorted = []\n",
        "for key in sorted(Dirs_and_Scores.keys()) :\n",
        "   #print(key , \" :: \" , Dirs_and_Scores[key])\n",
        "   #Dirs_and_Scores[key]\n",
        "   Image_Probs_Sorted.append(Dirs_and_Scores[key])\n",
        "\n",
        "#Sort Cross-Sectional Image Labels \n",
        "labels.sort()\n",
        "ImageLabels = labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uGtwTUAYzld"
      },
      "outputs": [],
      "source": [
        "# Save scores of known cross-sectional images\n",
        "H_CNN_Scores_Known = torch.tensor(Image_Probs_Sorted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "el9ZBhR8Yzle"
      },
      "source": [
        "## Forward-pass unknown (pseudo-novel) MIP images to extract their classification scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7STbpj5Yzle"
      },
      "outputs": [],
      "source": [
        "# Forward-Pass Images to the network to get (1) Softmax probabilities and (2) best classification prediction\n",
        "\n",
        "# Average Logits first, then pass the new logit (average) to the softmax function\n",
        "\n",
        "# Each instance is a subfolder containing a maximum intensity projection (MIP) image\n",
        "\n",
        "mean = np.array([0.5, 0.5, 0.5])\n",
        "std = np.array([0.25, 0.25, 0.25])\n",
        "\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    torchvision.transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "def predict(model, image_dir):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    patch_paths = glob.glob(os.path.join(image_dir, '*'))\n",
        "    images = []\n",
        "    for path in patch_paths:\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        image = transform(image)\n",
        "        images.append(image)\n",
        "\n",
        "    images = torch.stack(images, 0).to(device)\n",
        "    logits = model(images)\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    logits = torch.tensor(logits)\n",
        "    logits_mean = logits.mean(0)\n",
        "    probs = torch.softmax(logits_mean, 0).detach().cpu().numpy()\n",
        "    # Compute average and get final prediction\n",
        "    # avg_probs = probs.mean(0)\n",
        "    avg_probs = probs\n",
        "    final_pred = avg_probs.argmax()\n",
        "    return final_pred, avg_probs\n",
        "\n",
        "val_dir = absolute_path + \"/Pseudonovel_Oleifolius/MIP_Oleifolius/\"\n",
        "\n",
        "val_class_dirs = glob.glob(os.path.join(val_dir, '*'))\n",
        "model.eval()\n",
        "#class_map = {name: idx for idx, name in enumerate(class_names)}\n",
        "results = []\n",
        "all_preds = []\n",
        "labels = []\n",
        "all_image_dirs = []\n",
        "\n",
        "for d in val_class_dirs:\n",
        "    image_dirs = glob.glob(os.path.join(d, '*'))\n",
        "    for image_dir in image_dirs:\n",
        "        class_name = os.path.basename(image_dir).split('.')[0]\n",
        "        #label = class_map[class_name]\n",
        "        labels.append(label)\n",
        "        all_image_dirs.append(image_dir)  \n",
        "        class_id, probs = predict(model, image_dir)\n",
        "        all_preds.append(probs)\n",
        "        results.append((image_dir, class_id))\n",
        "\n",
        "all_preds = np.stack(all_preds, 0)\n",
        "labels = np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JWwWZ_CYzle"
      },
      "outputs": [],
      "source": [
        "# Concatenate Cross-Sectional Image Directories and Softmax Probabilities, and Get SORTED Probability vectors\n",
        "import numpy as np \n",
        "Dirs_and_Scores = {}\n",
        "for i in range (len(all_image_dirs)):\n",
        "  array_dir = all_image_dirs[i]\n",
        "  array_pred = all_preds[i]\n",
        "  Dirs_and_Scores[array_dir] = array_pred\n",
        "  #Dirs_and_Scores.append(Dir_and_Score)\n",
        "  \n",
        "Image_Probs_Sorted = []\n",
        "for key in sorted(Dirs_and_Scores.keys()) :\n",
        "   #print(key , \" :: \" , Dirs_and_Scores[key])\n",
        "   #Dirs_and_Scores[key]\n",
        "   Image_Probs_Sorted.append(Dirs_and_Scores[key])\n",
        "\n",
        "#Sort Cross-Sectional Image Labels \n",
        "labels.sort()\n",
        "ImageLabels = labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGf9jm_HYzle"
      },
      "outputs": [],
      "source": [
        "# Save scores of unknown (pseudo-novel) cross-sectional images\n",
        "H_CNN_Scores_Unknown = torch.tensor(Image_Probs_Sorted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkoRAMAyZdip"
      },
      "source": [
        "# Load P-CNN (trained on entire dataset with the exception of the pseudo-novel taxon\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXyvaf7vZdiq"
      },
      "outputs": [],
      "source": [
        "PATH = absolute_path + \"/models/P-CNN_Podocarpus_Split_04_WO_Oleifolius.pt\"\n",
        "model =  models.resnext101_32x8d(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, len(class_names)); \n",
        "model.load_state_dict(torch.load(PATH));\n",
        "#model.load_state_dict(torch.load(PATH, map_location=\"cpu\"))\n",
        "model.to(device);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-imk-ubDZdiq"
      },
      "source": [
        "## Forward-pass known patches to extract their classification scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBPjhZlYZdiq",
        "outputId": "f244901c-3c71-45d0-96e7-21aedfc50f2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(96, 29) float32 (96,) int64\n",
            "Accuracy: 0.7708333333333334\n"
          ]
        }
      ],
      "source": [
        "# Forward Pass Images to the network to get (1) Softmax probabilities and (2) best classification prediction\n",
        "\n",
        "# Average Logits first, then pass the new logit (average) to the softmax function\n",
        "\n",
        "# Each instance is a subfolder containing multiple (N) patches (patches extracted from *one* image)\n",
        "\n",
        "\n",
        "mean = np.array([0.5, 0.5, 0.5])\n",
        "std = np.array([0.25, 0.25, 0.25])\n",
        "\n",
        "\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    torchvision.transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "def predict(model, image_dir):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    patch_paths = glob.glob(os.path.join(image_dir, '*'))\n",
        "    images = []\n",
        "    for path in patch_paths:\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        image = transform(image)\n",
        "        images.append(image)\n",
        "\n",
        "    images = torch.stack(images, 0).to(device)\n",
        "    logits = model(images)\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    logits = torch.tensor(logits)\n",
        "    logits_mean = logits.mean(0)\n",
        "    probs = torch.softmax(logits_mean, 0).detach().cpu().numpy()\n",
        "    # Compute average and get final prediction\n",
        "    # avg_probs = probs.mean(0)\n",
        "    avg_probs = probs\n",
        "    final_pred = avg_probs.argmax()\n",
        "    return final_pred, avg_probs\n",
        "\n",
        "val_dir = absolute_path + \"/Patches_Podocarpus_WO_Oleifolius_Train_Val/val\"\n",
        "\n",
        "val_class_dirs = glob.glob(os.path.join(val_dir, '*'))\n",
        "model.eval()\n",
        "class_map = {name: idx for idx, name in enumerate(class_names)}\n",
        "results = []\n",
        "all_preds = []\n",
        "labels = []\n",
        "all_image_dirs = []\n",
        "\n",
        "for d in val_class_dirs:\n",
        "    image_dirs = glob.glob(os.path.join(d, '*'))\n",
        "    for image_dir in image_dirs:\n",
        "        class_name = os.path.basename(image_dir).split('.')[0]\n",
        "        label = class_map[class_name]\n",
        "        labels.append(label)\n",
        "        all_image_dirs.append(image_dir)  \n",
        "        class_id, probs = predict(model, image_dir)\n",
        "        all_preds.append(probs)\n",
        "        results.append((image_dir, class_id))\n",
        "        #print(f'Predict {image_dir} as class {class_id}')\n",
        "\n",
        "all_preds = np.stack(all_preds, 0)\n",
        "labels = np.array(labels)\n",
        "print(all_preds.shape, all_preds.dtype, labels.shape, labels.dtype)\n",
        "print('Accuracy: {}'.format((all_preds.argmax(1) == labels).mean()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6KAfNvbZdiq"
      },
      "outputs": [],
      "source": [
        "# Concatenate Cross-Sectional Image Directories and Softmax Probabilities, and Get SORTED Probability vectors\n",
        "import numpy as np \n",
        "Dirs_and_Scores = {}\n",
        "for i in range (len(all_image_dirs)):\n",
        "  array_dir = all_image_dirs[i]\n",
        "  array_pred = all_preds[i]\n",
        "  Dirs_and_Scores[array_dir] = array_pred\n",
        "  #Dirs_and_Scores.append(Dir_and_Score)\n",
        "  \n",
        "Patch_Probs_Sorted = []\n",
        "for key in sorted(Dirs_and_Scores.keys()) :\n",
        "   #print(key , \" :: \" , Dirs_and_Scores[key])\n",
        "   #Dirs_and_Scores[key]\n",
        "   Patch_Probs_Sorted.append(Dirs_and_Scores[key])\n",
        "\n",
        "#Sort Cross-Sectional Image Labels \n",
        "labels.sort()\n",
        "PatchLabels = labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlDhJaJCZdir"
      },
      "outputs": [],
      "source": [
        "# Save scores of known cross-sectional images\n",
        "P_CNN_Scores_Known = torch.tensor(Patch_Probs_Sorted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chiaCS9kZdir"
      },
      "source": [
        "## Forward-pass unknown patches to extract their classification scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkLuRAMcZdir"
      },
      "outputs": [],
      "source": [
        "# Forward Pass Images to the network to get (1) Softmax probabilities and (2) best classification prediction\n",
        "\n",
        "# Average Logits first, then pass the new logit (average) to the softmax function\n",
        "\n",
        "# Each instance is a subfolder containing multiple (N) patches (patches extracted from *one* image)\n",
        "\n",
        "mean = np.array([0.5, 0.5, 0.5])\n",
        "std = np.array([0.25, 0.25, 0.25])\n",
        "\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    torchvision.transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "def predict(model, image_dir):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    patch_paths = glob.glob(os.path.join(image_dir, '*'))\n",
        "    images = []\n",
        "    for path in patch_paths:\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        image = transform(image)\n",
        "        images.append(image)\n",
        "\n",
        "    images = torch.stack(images, 0).to(device)\n",
        "    logits = model(images)\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    logits = torch.tensor(logits)\n",
        "    logits_mean = logits.mean(0)\n",
        "    probs = torch.softmax(logits_mean, 0).detach().cpu().numpy()\n",
        "    # Compute average and get final prediction\n",
        "    # avg_probs = probs.mean(0)\n",
        "    avg_probs = probs\n",
        "    final_pred = avg_probs.argmax()\n",
        "    return final_pred, avg_probs\n",
        "\n",
        "val_dir = absolute_path + \"/Pseudonovel_Oleifolius/Cross_Sections_Oleifolius/\"\n",
        "\n",
        "val_class_dirs = glob.glob(os.path.join(val_dir, '*'))\n",
        "model.eval()\n",
        "#class_map = {name: idx for idx, name in enumerate(class_names)}\n",
        "results = []\n",
        "all_preds = []\n",
        "labels = []\n",
        "all_image_dirs = []\n",
        "\n",
        "for d in val_class_dirs:\n",
        "    image_dirs = glob.glob(os.path.join(d, '*'))\n",
        "    for image_dir in image_dirs:\n",
        "        class_name = os.path.basename(image_dir).split('.')[0]\n",
        "        #label = class_map[class_name]\n",
        "        labels.append(label)\n",
        "        all_image_dirs.append(image_dir)  \n",
        "        class_id, probs = predict(model, image_dir)\n",
        "        all_preds.append(probs)\n",
        "        results.append((image_dir, class_id))\n",
        "\n",
        "all_preds = np.stack(all_preds, 0)\n",
        "labels = np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6T56IOphZdir"
      },
      "outputs": [],
      "source": [
        "# Concatenate Cross-Sectional Image Directories and Softmax Probabilities, and Get SORTED Probability vectors\n",
        "import numpy as np \n",
        "Dirs_and_Scores = {}\n",
        "for i in range (len(all_image_dirs)):\n",
        "  array_dir = all_image_dirs[i]\n",
        "  array_pred = all_preds[i]\n",
        "  Dirs_and_Scores[array_dir] = array_pred\n",
        "  #Dirs_and_Scores.append(Dir_and_Score)\n",
        "  \n",
        "Patch_Probs_Sorted = []\n",
        "for key in sorted(Dirs_and_Scores.keys()) :\n",
        "   #print(key , \" :: \" , Dirs_and_Scores[key])\n",
        "   #Dirs_and_Scores[key]\n",
        "   Patch_Probs_Sorted.append(Dirs_and_Scores[key])\n",
        "\n",
        "#Sort Cross-Sectional Image Labels \n",
        "labels.sort()\n",
        "PatchLabels = labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4-nV2WaZdir"
      },
      "outputs": [],
      "source": [
        "# Save scores of unknown (pseudo-novel) cross-sectional images\n",
        "P_CNN_Scores_Unknown = torch.tensor(Patch_Probs_Sorted)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute fused scores"
      ],
      "metadata": {
        "id": "6nRFcJgukQ0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Fused Classification_Scores (element-wise multiplication) - Known specimens \n",
        "Fused_Scores_Known = C_CNN_Scores_Known * H_CNN_Scores_Known * P_CNN_Scores_Known"
      ],
      "metadata": {
        "id": "_SK1WyFGZcur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Fused Classification_Scores (element-wise multiplication) - unknwon (pseudo-novel) specimens \n",
        "Fused_Scores_Unknown = C_CNN_Scores_Unknown * H_CNN_Scores_Unknown * P_CNN_Scores_Unknown"
      ],
      "metadata": {
        "id": "Lp4aMF4dkaN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize (0-1) fused scores - Known specimens\n",
        "import torch\n",
        "import torch.nn.functional as f\n",
        "Normalized_Prob_Fused_Scores_Known = f.normalize(torch.tensor(Fused_Scores_Known), p=2, dim=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBPHMB5OZc19",
        "outputId": "110c7a5d-b779-4274-bd6b-4b44d1dad65c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  after removing the cwd from sys.path.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize (0-1) fused scores - Unknown specimens\n",
        "import torch\n",
        "import torch.nn.functional as f\n",
        "Normalized_Prob_Fused_Scores_Unknown = f.normalize(torch.tensor(Fused_Scores_Unknown), p=2, dim=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TmFFlTYlLfd",
        "outputId": "3c726a50-010c-47af-cc88-b5b6b4adff7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  after removing the cwd from sys.path.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Define Shannon Entropy"
      ],
      "metadata": {
        "id": "bfVzzx2SlY4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shannon Entropy \n",
        "def myEntropy(p, dim = -1, keepdim=False):\n",
        "    return -torch.where(p > 0, p * torch.log(p), p.new([0.0])).sum(dim = dim, keepdim=keepdim) "
      ],
      "metadata": {
        "id": "XWf5OYzAlX3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute entropy across all three modalities for both Known and Unknown specimens"
      ],
      "metadata": {
        "id": "_hCsk0IVllMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Entropy_C_CNN_Known = myEntropy(C_CNN_Scores_Known)\n",
        "Entropy_H_CNN_Known = myEntropy(H_CNN_Scores_Known)\n",
        "Entropy_P_CNN_Known = myEntropy(P_CNN_Scores_Known)\n",
        "Entropy_FM_Known = myEntropy(Normalized_Prob_Fused_Scores_Known)"
      ],
      "metadata": {
        "id": "EwcjcH15lsDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Entropy_C_CNN_Unknown = myEntropy(C_CNN_Scores_Unknown)\n",
        "Entropy_H_CNN_Unknown = myEntropy(H_CNN_Scores_Unknown)\n",
        "Entropy_P_CNN_Unknown = myEntropy(P_CNN_Scores_Unknown)\n",
        "Entropy_FM_Unknown = myEntropy(Normalized_Prob_Fused_Scores_Unknown)"
      ],
      "metadata": {
        "id": "vjirUt0TlsGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDeZayCn1Xw6"
      },
      "source": [
        "Plot the ROC curves - In this example, we generated the ROC curve based on the fused model (FM). This model combines classification scores across all three modalities. The curve shows the performance of our model (i.e. its ability to differentiates between known versus unknown pollen specimens) at different entropy thresholds."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_curve, roc_auc_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def evaluate_openset(scores_closeset, scores_openset):    \n",
        "    y_true = np.array([0] * len(scores_closeset) + [1] * len(scores_openset))\n",
        "    y_discriminator = np.concatenate([scores_closeset, scores_openset])\n",
        "    auc_d, roc_to_plot = plot_roc(y_true, y_discriminator, 'Discriminator ROC')\n",
        "    return auc_d, roc_to_plot\n",
        "\n",
        "\n",
        "def plot_roc(y_true, y_score, title=\"Receiver Operating Characteristic\", **options):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
        "    auc_score = roc_auc_score(y_true, y_score)\n",
        "    roc_to_plot = {'tp':tpr, 'fp':fpr, 'thresh':thresholds, 'auc_score':auc_score}\n",
        "    return auc_score, roc_to_plot\n",
        "\n",
        "predConfscores_known = -np.array(Entropy_FM_Known)\n",
        "predConfscores_novel = -np.array(Entropy_FM_Unknown)\n",
        "\n",
        "roc_score, roc_to_plot = evaluate_openset(-predConfscores_known, -predConfscores_novel)\n",
        "\n",
        "plt.figure(figsize=(9,8), dpi=64, facecolor='w', edgecolor='k')\n",
        "ax = plt.gca()\n",
        "plt.plot(roc_to_plot['fp'], roc_to_plot['tp'], linewidth=2)\n",
        "\n",
        "for tick in ax.xaxis.get_major_ticks():\n",
        "    tick.label.set_fontsize(12) \n",
        "for tick in ax.yaxis.get_major_ticks():\n",
        "    tick.label.set_fontsize(12) \n",
        "plt.grid('on')\n",
        "plt.xlabel('False Positive Rate', fontsize=15)\n",
        "plt.ylabel('True Positive Rate', fontsize=15)\n",
        "plt.title('ROC score {:.5f}'.format(roc_score), fontsize=15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "pVWJvZKaoMuT",
        "outputId": "eeae16e4-0d1a-4f11-fcf0-e48f57002c77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'ROC Score 0.93939')"
            ]
          },
          "metadata": {},
          "execution_count": 58
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x512 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfMAAAG/CAYAAABMlT5cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAJ1wAACdcBsW4XtwAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de1hVdb7H8Q9ichcUR0xAGAXMUKEUEy0vhNOEF+g2qZQ6YebTdHLG0rGTR5u0tMmyZqa0SdMayeyi2MyxRuZo5iQ1XsLSUJBEQRGNJETxAvzOHz7uJwLcW2FvWPp+PQ+Pe63927/13d/MD2vttddyM8YYAQAAy2rV3AUAAIDGIcwBALA4whwAAIsjzAEAsDjCHAAAiyPMAQCwOMIcuERDhgxRmzZt5Ovrq7Zt2yo6OlqvvfZanXH79u1TamqqgoKC5OXlpa5du2r69OmqqKioM3b58uWKj4+Xn5+f2rVrp169eumpp57SDz/8UG8NlZWVmjZtmn7+85/L19dXHTp00M0336yNGzc2+fttrPfff1/XXXedvLy81KNHD61evfqi448fP66HHnpIwcHB8vX11bBhw7Rnzx7b8xUVFRo6dKiCgoLUtm1bhYaG6ne/+51Onz5tG3P69GnNmDFD4eHh8vX1Vf/+/ZWVlVVrO6NGjVJwcLDatm2ra6+9Vr/+9a9VWlpqe76mpkbPP/+8IiIi5Ovrq169eukf//hHE3UFaGIGwCUZPHiwefLJJ40xxlRXV5tVq1YZNzc388knn9jG7Nq1y/j7+5vx48ebgoICU1VVZXbs2GH69etnbrzxRnPy5Enb2IceesgEBQWZt99+2xw/ftwYY8yePXvMb37zG/Ppp5/WW8PkyZPNTTfdZPbt22eMMaa8vNysW7fOZGVlOeU9nzlz5rJe9/nnnxsPDw/z/vvvm7Nnz5r333/feHp6mq1btzb4mlGjRpnbbrvNHDt2zFRWVpopU6aYkJAQU1FRYYwx5uzZs+arr76y1VRcXGwGDRpkpk6dapvj0UcfNTfccIM5cOCAOXv2rFmwYIHx9fU1RUVFtjHZ2dnm1KlTxhhjvv/+e3PvvfeaO++80/b8iy++aLp06WJ27dplqqqqzMqVK80111xjtm/fflm9AJyJMAcu0Y/D/ILAwEDz/PPP25aHDRtmbr755jqvPXbsmPH39zfz5s0zxhjz2WefGUnmX//61yXV0LNnT/PCCy9cdExhYaEZO3asCQ4ONn5+fiYmJsYWRJWVlWb69OkmPDzcBAQEmJtvvtl8/vnnttcuW7bMBAcHm7/85S8mLCzM+Pr6GmOMKSoqMmPGjDGdO3c2P/vZz8zo0aPN0aNHG6xhwoQJJiUlpda6lJQU88ADD9Q7vqKiwrRq1arWLyWVlZXG3d3drFixot7XFBcXmyFDhpjhw4fb1nXs2NGsXLmy1rjg4GAzd+7ceuf4/vvvzZgxY0x0dLRtXb9+/Wz/nS4YOHCgmThxYr1zAM2Jw+xAI1RVVentt99WaWmpevToIen8IfANGzZo/PjxdcZ36NBBw4cPtx2u/d///V917txZt9566yVtd8iQIXr++ef1wgsvaMuWLTp16lSt5ysrK5WQkKA2bdooOztbZWVlevvttxUYGChJmjZtmtatW6fMzEyVlJQoJSVFiYmJKioqss1x5MgR7dy5U7t27VJJSYnOnDmjW2+9VZ07d1Zubq6+/fZbtW7dWmPHjm2wzuzsbPXr16/Wuri4OH355ZcNvsac38mos7xjx45a41JTU+Xj46Nrr71WO3fu1PTp0xuc48K6n87xxBNPyM/PT+3bt1dGRoZmz5590TlqamrqzAG0CM32awRgUYMHDzYeHh7G39/fuLu7G3d3d/Pcc8/Zni8qKjKSzLp16+p9/fTp001kZKQxxpiJEyeafv36XXINZ8+eNYsWLTKJiYnG39/feHh4mDvvvNMUFhYaY4x57733TPv27c3p06frvLa6utp4eXmZjIyMWut79+5t2xNdtmyZcXd3r/VxwAcffGA6d+5sampq6rzXC9v9qa5du5pXX3211rpXX33VdOvWrcH3NmzYMJOYmGiOHDliKioqzCOPPGLc3Nzq3SOuqakx2dnZZsaMGWb//v229Q8++KCJiYkx+fn55vTp02b+/PnGzc3NJCYm1rvNvLw88+STT5rs7GzbumeeecaEhISY7Oxsc/bsWbNixQrTqlUrExER0WDtQHNhzxy4DI8//rjKysp0/PhxTZgwQevXr1dVVZUkqX379nJ3d9ehQ4fqfW1RUZE6duwoSerYsWOtvWFHXXPNNZo8ebIyMzN1/Phxbd68Wfv27dN9990nSdq/f7/Cw8Pl4eFR57XfffedKisr1a1bt1rrIyIidPDgQdtyx44d5e3tbVvOy8tTSUmJ2rVrp4CAAAUEBCg6OloeHh61Xvdjbdu2VVlZWa11x48fV9u2bRt8bytWrFDnzp3Vp08fRUREqF27drruuuvUoUOHOmPd3NwUExOjG264QXfddZdt/YsvvqjBgwcrISFBISEh2r9/v2699dZ657jw3keNGqXbbrtN586dkyRNnz5daWlpuvvuuxUUFKS1a9dqzJgxDc4BNCfCHGgEPz8/vfLKK/r222/1yiuvSJK8vLw0dOhQ/e1vf6szvrS0VOvWrdPw4cMlScOHD9fhw4e1YcOGy67Bzc1NcXFxmjhxou0QcHh4uAoKCnT27Nk64zt06CBPT0/l5+fXWp+fn68uXbrYllu1qv3PQ6dOnRQWFqaysrJaP6dPn9aAAQPqrS02NlZbt26ttW7btm264YYbGnw/HTt21JtvvqmioiIVFxfrkUcesYVxQ86dO6e9e/faln19ffXyyy+roKBAx44d00svvaTdu3fbnaOkpMT2DYLWrVvrqaeeUl5enr7//nu9++67+uabby75IxHAJZr70ABgNfWdALds2TITGBhoysrKjDHGfPXVV6Zt27bmgQceMAcPHjRVVVXmyy+/NP379zcxMTG2M7ONOX82e6dOncyqVatsr8/LyzNTpkxp8Gz2WbNmmQ0bNpjy8nJjzPmz3/v06WM7CezUqVOmW7duJi0tzRw7dszU1NSY3bt3m4KCAmOMMQ8//LDp3bu3yc/PN2fOnDEvvvii8fHxMQcPHrS9n+Dg4FrbLC8vN6GhoWbmzJm2OktKSsw777zTYK+ysrKMh4eHWb16tTl79qxZvXq18fT0NP/5z38afM2ePXtMSUmJrQ+JiYkmKSnJ9vwXX3xh1q9fb06ePGmqq6vNtm3bTGRkpLnnnntsY/bv32879H/o0CEzduxYc8MNN9g+dti7d6/54IMPzA8//GBqamrMnj17THx8vImLi7PNceTIEZOXl2dqamrMd999Z6ZOnWpCQ0PNd99912DtQHMhzIFLVF+YV1VVmaioKPPEE0/Y1u3Zs8eMHj3adOjQwXh6eprw8HDz2GOPmR9++KHOnG+88Ybp37+/8fHxMQEBAaZnz57mD3/4Q71jjTHm2WefNX369DEBAQHG19fXhIWFmYcfftiUlpbaxhw4cMDce++9plOnTsbPz8/ExsaaHTt2GGPOh/3jjz9uunTpYvz9/c3AgQPNli1bbK+tL8yNOf8Z+bhx40xoaKjx8/Mz3bp1Mw8//PBF+/Xuu++a7t27Gw8PD9O9e3fz/vvv13r++uuvN88884xteenSpSY4ONh4eXmZkJAQM23aNFNZWWl7fvPmzaZv376mbdu2xtfX13Tr1s08/vjjtl9sjDFm3bp1Jjw83Hh5eZmOHTuaSZMmme+//972/J49e8zAgQONv7+/8fHxMWFhYWbSpEmmuLjYNmbHjh2me/fuxsfHx7Rr186MHj3a9ssO0NK4GcP9zAEAsDI+MwcAwOIIcwAALI4wBwDA4ghzAAAsrnVzF3C5unfvXueiF41VXl5+0YtZwD562Hj0sPHoYePRw8ZzRg/z8/NrXVPhAsuGebdu3bRu3bomnTMrK0vx8fFNOufVhh42Hj1sPHrYePSw8ZzRw6SkpHrXc5gdAACLI8wBALA4whwAAIsjzAEAsDjCHAAAiyPMAQCwOMIcAACLI8wBALA4whwAAIsjzAEAsDjCHAAAiyPMAQCwOKeH+UMPPaTg4GC5ubk1OKa4uFgJCQmKiopSXFyccnJynF0WAABXDKeHeWpqqnbs2HHRMTNmzFBycrJyc3M1c+ZMTZ482dllAQBwxXB6mA8aNEhBQUEXHbNmzRqlpaVJkkaNGqXc3FwdPXrU2aUBAHBFaPb7mZeWlsrLy0u+vr6SJDc3N4WGhqqwsFAdO3Zs5urQkE25x7QwM1cnz1TVWn+qslLeWZuaqaorAz1sPHrYePSw8U5VVqpH7lYtGR/n9G01e5hfivT0dKWnp0uS8vPzlZWV1aTzl5WVNfmcV6o5n/6gfcer63/yRIVri7kS0cPGo4eNRw8br6jUJbnS7GEeGBioyspKnTx5Uj4+PjLGqLCwUKGhoXXGpqamKjU1VZKUlJSk+Pj4Jq0lKyuryee8UrllbZJUoTburRQW6G1bf6qyUt5eXs1X2BWAHjYePWw8eth4pyor1SMkUPHxV8meeUpKipYsWaIpU6boww8/VGRkJIfYLSIs0FuZUwfblvmFqPHoYePRw8ajh413vofOD3LJBSfATZgwQSEhIZKkkJAQ3X///Tp8+LBiY2NtY+bPn6+MjAxFRUVpzpw5Wrx4sbPLAgDgiuH0PfPly5fXuz47O9v2uHPnztq4caOzSwEA4IrEFeAAALA4whwAAIsjzAEAsDjCHAAAiyPMAQCwOMIcAACLI8wBALA4whwAAIsjzAEAsDjCHAAAiyPMAQCwOMIcAACLI8wBALA4whwAAIsjzAEAsDjCHAAAiyPMAQCwOMIcAACLI8wBALA4whwAAIsjzAEAsDjCHAAAiyPMAQCwOMIcAACLI8wBALA4whwAAItr3dwFoHE25R7TwsxcnTxT5dLtHig95dLtAQAaRphb3MLMXGUXljXb9n08+CsEAM2Nf4kt7sIeeRv3VgoL9Hbptn08Wut3w6Jcuk0AQF2E+RUiLNBbmVMHN3cZAIBmwAlwAABYHGEOAIDFEeYAAFgcYQ4AgMUR5gAAWBxhDgCAxRHmAABYHGEOAIDFEeYAAFgcYQ4AgMUR5gAAWBxhDgCAxRHmAABYHGEOAIDFEeYAAFgcYQ4AgMUR5gAAWBxhDgCAxRHmAABYHGEOAIDFEeYAAFgcYQ4AgMUR5gAAWBxhDgCAxRHmAABYHGEOAIDFEeYAAFgcYQ4AgMUR5gAAWBxhDgCAxRHmAABYHGEOAIDFuSTMc3JyFBcXp6ioKCUkJKi4uLjOmPz8fA0ePFixsbGKjo7WkiVLXFEaAACW55Iwnzx5smbOnKnc3FwlJydrxowZdcb8z//8j+69915lZ2drw4YNmjp1qk6cOOGK8gAAsDSnh3lJSYny8vKUnJwsSUpLS9OaNWvqFtKqlX744QdJUkVFhdq1aycPDw9nlwcAgOW1dvYGioqKFBoaalv29fWVp6enSktLFRgYaFs/f/58JSUl6ZVXXlFZWZneffddtWnTptZc6enpSk9Pl3T+sHxWVlaT1lpWVtbkczrbqcpK258toXYr9rCloYeNRw8bjx42nit76PQwd9SiRYv0yCOPaNKkSdq5c6dGjBihb775Rn5+frYxqampSk1NlSQlJSUpPj6+SWvIyspq8jmdzTtrk3SiQt5eXi2idiv2sKWhh41HDxuPHjaeK3vo9DAPCQlRYWGhbbmiokKnT5+utVcuSS+//LKOHj0qSYqJidG1116rnJwc9evXz9klAgBgaU7/zDwoKEgRERFau3atJGnp0qVKSUmpMy4sLEyZmZmSpAMHDqigoEDdunVzdnkAAFieS85mX7RokebMmaPIyEhlZGRo/vz5kqTY2FgdPnxYkvTGG2/omWeeUUxMjEaMGKG//OUvdfbeAQBAXS75zDw6Olrbtm2rsz47O9v2+KabbtJ//vMfV5QDAMAVhSvAAQBgcYQ5AAAWR5gDAGBxhDkAABZHmAMAYHGEOQAAFkeYAwBgcYQ5AAAWR5gDAGBxhDkAABZHmAMAYHGEOQAAFkeYAwBgcYQ5AAAWR5gDAGBxhDkAABZHmAMAYHGEOQAAFkeYAwBgcYQ5AAAWR5gDAGBxhDkAABZHmAMAYHGEOQAAFkeYAwBgcYQ5AAAWR5gDAGBxhDkAABZHmAMAYHGEOQAAFkeYAwBgcYQ5AAAWR5gDAGBxhDkAABZHmAMAYHGEOQAAFkeYAwBgcYQ5AAAWR5gDAGBxhDkAABZHmAMAYHGEOQAAFudQmG/ZskWTJk3SyJEjJUk7duzQv//9b6cWBgAAHGM3zFetWqXbb79dkrRp0yZJUk1NjWbNmuXcygAAgEPshvncuXP10Ucf6a9//avc3d0lSb169dKuXbucXhwAALDPbpgXFhZqwIABkiQ3NzdJUps2bVRVVeXcygAAgEPshnl4eLi+/PLLWuu2b9+url27Oq0oAADgOLthPnXqVN1xxx16/fXXVVVVpRUrVig1NVWPPfaYK+oDAAB2tLY3YNy4caqpqdHChQtVVVWl2bNna8qUKRozZowr6gMAAHbYDXNJmjBhgiZMmODkUgAAwOWwe5i9V69e9a6PjY1t8mIAAMCls7tnXlBQUO/6AwcONHUtV7xNuce0MDNXJ8803TcBDpSearK5AADW1GCYv/HGG5Kk6upqLVu2TMYY23N79+5VUFCQ86u7wizMzFV2YZlT5vbxcOgTEwDAFajBBJgzZ44k6cyZM3r66adt61u1aqVOnTrp5Zdfdn51V5gLe+Rt3FspLNC7yeb18Wit3w2LarL5AADW0mCY79+/X5KUlJSkdevWuaygq0FYoLcypw5u7jIAAFcIuyfAEeQAALRsDn3QmpmZqfXr1+vo0aO1Pjt/6623nFYYAABwjN0981dffVUjR45UXl6eVq1apfLycr3//vuqrq52RX0AAMAOu2H+5z//WWvWrFFGRoa8vLyUkZGhN998U/7+/q6oDwAA2GE3zA8dOmS7n/mFQ+x33nmnVq9e7dzKAACAQ+yGedu2bXXixAlJUlBQkPbt26fy8nKdOsXFSgAAaAnshvmAAQNse+EjR47UyJEjNXToUA0aNMjhjeTk5CguLk5RUVFKSEhQcXFxveNefPFFXXfdderVq5duu+02h+cHAOBqZvds9r/97W+2w+vPPvusAgMDVV5ermnTpjm8kcmTJ2vmzJlKTk7Wyy+/rBkzZujNN9+sNea9997Txx9/rC+//FJeXl4qKSm5xLcCAMDVye6euYeHhzw9PSVJbdq00RNPPKF58+Zpx44dDm2gpKREeXl5Sk5OliSlpaVpzZo1dcYtXLhQc+fOlZeXlyRxuVgAABx00T3zkydPau/evQoPD1f79u0lSTt37tS0adO0adMmnTlzxu4GioqKFBoaalv29fWVp6enSktLFRgYaFufk5OjjRs36tFHH5UkTZs2TXfddVetudLT05Weni5Jys/PV1ZWloNv0zFlZWVNPuePnaqstP3pzO00J2f38GpADxuPHjYePWw8V/awwTDftGmTkpOTVV5eLi8vL61Zs0ZZWVmaN2+exowZoz179jRpIVVVVTp69KiysrJUVFSkAQMGKCYmRhEREbYxqampSk1NlXT+MrPx8fFNWkNWVlaTz/lj3lmbpBMV8vbycup2mpOze3g1oIeNRw8bjx42nit72GCYz5w5Uw888IDS0tL02muv6f7771d4eLiys7N13XXXObyBkJAQFRYW2pYrKip0+vTpWnvlktSlSxeNHj1abm5uCg0NVXx8vLKzs2uFOQAAqKvBz8xzcnI0b948RUdHa968eTp27Jg++OCDSwpy6fxn3xEREVq7dq0kaenSpUpJSakzbvTo0Vq/fr0k6fjx49q2bZuio6MvaVsAAFyNGgzzs2fPysPDQ5Lk4+Mjf39/hYSEXNZGFi1apDlz5igyMlIZGRmaP3++JCk2NlaHDx+WJD322GPatWuXevbsqUGDBmnmzJnq0aPHZW0PAICrSYOH2aurq7Vx40bb19J+uixJCQkJDm0kOjpa27Ztq7M+Ozvb9tjb21srV650uHAAAHBeg2FeWVmpW2+9tda6Hy+7ublxsxUAAFqABsO8pqbGlXUAAIDLZPeiMQAAoGUjzAEAsDjCHAAAiyPMAQCwOMIcAACLcyjMt2zZokmTJmnkyJGSpB07dujf//63UwsDAACOsRvmq1at0u233y7p/M1XpPNfW5s1a5ZzKwMAAA6xG+Zz587VRx99pL/+9a9yd3eXJPXq1Uu7du1yenEAAMA+u2FeWFioAQMGSDp/1TdJatOmjaqqqpxbGQAAcIjdMA8PD9eXX35Za9327dvVtWtXpxUFAAAcZzfMp06dqjvuuEOvv/66qqqqtGLFCqWmpuqxxx5zRX0AAMCOBq/NfsG4ceNUU1OjhQsXqqqqSrNnz9aUKVM0ZswYV9RnKZtyj2lhZq5Onqn/I4gDpadcXBEA4GpgN8wlacKECZowYYKTS7G+hZm5yi4sszvOx8OhtgMA4BC7qZKSkqIHH3xQSUlJthPgUL8Le+Rt3FspLNC73jE+Hq31u2FRriwLAHCFsxvmXbp00bhx4+Tl5aUJEyYoLS1NP//5z11Rm2WFBXorc+rg5i4DAHCVsHsC3J/+9CcdPnxYzz33nLZs2aLIyEglJiZq1apVrqgPAADY4dDlXD08PJSamqoNGzZo79698vb21tixY51dGwAAcIDDZ2JVVFRo5cqVWrJkibKzs5WSkuLMugAAgIPshvlnn32mpUuX6r333lNwcLDS0tL097//XR07dnRFfQAAwA67Yf6LX/xC99xzj9atW6dbbrnFFTUBAIBLYDfMi4uL1bZtW1fUAgAALkO9YV5QUKDw8HBJ0nfffafvvvuu3hdzfXYAAJpfvWHeu3dvlZeXS5IiIiLqXCzGGCM3NzdVV1c7v0IAAHBR9Yb57t27bY/379/vsmIAAMClqzfMQ0NDbY+Lioo0cODAOmO2bNmisLAw51UGAAAcYveiMbfffnu960eMGNHkxQAAgEtnN8yNMXXWnTlzhpuuAADQQjT41bRbbrlFbm5uOn36tAYNGlTruYMHD6pv375OLw4AANjXYJgnJiZKkr744gvdeuuttvWtWrVSp06ddO+99zq/OgAAYFeDYT579mxJUmRkJDdVAQCgBas3zC98j1ySRo8erZqamnpf3KqVQzddAwAATlRvmPv7+9suGtO6desGT3bjojEAADS/esN83bp1tscbNmzgzHUAAFqwesP85ptvtj0eMmSIq2oBAACXwe6H3qtXr9aePXskSfn5+brllls0dOhQffvtt04vDgAA2Gc3zP/7v/9bPj4+tsehoaHq2rWrpkyZ4vTiAACAfQ7dzzw0NFTGGP3rX/9Sfn6+PD09a12/HQAANB+7Yd6mTRudOnVK33zzjUJDQxUQEKDq6mqdOXPGFfUBAAA77IZ5YmKi7r33XpWWliolJUWStHfvXnXq1MnpxQEAAPvsfma+ePFixcTE6Je//KV+//vfSzp/Itwjjzzi9OIAAIB9dvfM/f39NXfu3FrrRo4c6bSCAADApXHoeqwffvihkpKS1LNnTyUlJWnt2rXOrgsAADjIbpi/9dZbSk1NVVRUlCZPnqyoqCiNGzdOb775pivqAwAAdtg9zL5gwQJlZGTUug3qqFGjNGXKFI0fP96pxbVkm3KPaWFmrk6eqbKtO1B6qhkrAgBcreyG+cGDB5WQkFBr3ZAhQ3Tw4EGnFWUFCzNzlV1YVu9zPh522woAQJOxmzqhoaHatGlTrWu0b968WSEhIc6sq8W7sEfexr2VwgK9bet9PFrrd8OimqssAMBVyG6YP/bYY0pOTtbEiRPVrVs35efn64033tALL7zgivpavLBAb2VOHdzcZQAArmJ2w3zChAny8/PT66+/ro8//lihoaF6/fXXdffdd7uiPgAAYMdFwzw/P19fffWVbrzxRn388ceuqgkAAFyCBr+a9uGHH6pHjx6666671KNHD8IcAIAWqsEwnzt3rp5++mmdOHFCs2bN0rPPPuvKugAAgIMaDPN9+/Zp2rRp8vHx0eOPP67c3FxX1gUAABzUYJhXVVXJ3d1d0vnboJ49e9ZlRQEAAMc1eALcuXPntGzZMhljJElnz56ttSxJDzzwgPMrBAAAF9VgmAcFBenpp5+2Lf/sZz+rtezm5kaYAwDQAjQY5gUFBS4sAwAAXC6HboEKAABaLsIcAACLI8wBALA4l4R5Tk6O4uLiFBUVpYSEBBUXFzc4dsWKFXJzc9Mnn3ziitIAALA8l4T55MmTNXPmTOXm5io5OVkzZsyod9zRo0e1aNEi9e/f3xVlAQBwRXAozFeuXKlhw4apd+/eks7fzzwjI8OhDZSUlCgvL0/JycmSpLS0NK1Zs6besf/1X/+lefPmycPDw6G5AQCAA2H+5z//WTNmzNDQoUN14MABSVL79u31xz/+0aENFBUVKTQ01Lbs6+srT09PlZaW1hq3evVq+fv7a9CgQZdSPwAAVz279zP/y1/+oo8++kjXX3+9nn/+eUnSdddd16TXaj9+/Ljmzp2rDRs2XHRcenq60tPTJZ2/PWtWVlaT1SBJZWVlDs95qrLS9mdT12Fll9JD1I8eNh49bDx62Hiu7KHdMD927Jiuv/56Seev+nbBjy/rejEhISEqLCy0LVdUVOj06dMKDAy0rdu9e7eKiooUGxsrSTpy5IhGjx6txYsXKyUlxTYuNTVVqampkqSkpCTFx8c7VIOjsrKyHJ7TO2uTdKJC3l5eTV6HlV1KD1E/eth49LDx6GHjubKHdg+zR0VF1TmzfNOmTerRo4dDGwgKClJERITWrl0rSVq6dGmtgJakm2++WUePHlVBQYEKCgrUv39/vfPOO3XGAQCAuuyG+axZs3THHXfoySef1NmzZzV37lyNGTNGs2bNcngjixYt0pw5cxQZGamMjAzNnz9fkhQbG6vDhw9ffvUAAMD+YfakpCStXr1aL730krp06aINGzbor9UO+5sAABWdSURBVH/9q37xi184vJHo6Ght27atzvrs7Ox6x/MdcwAAHGc3zCVp6NChGjp0qLNrAQAAl8FumH/77bcNPte1a9cmLQYAAFw6u2EeEREhNzc329nrPz6jvbq62nmVAQAAh9gN8/3799daPnTokJ5++mmNGzfOaUUBAADH2Q3zsLCwOstvvvmmbrvtNo0dO9ZphQEAAMdc1o1WAgICLvpZOgAAcB27e+Y/vcTqyZMntXz5cvXq1ctpRTWHTbnHNOfTH+SWtcmh8QdKTzm5IgAAHGM3zBMTE2st+/r6qm/fvlqyZInTimoOCzNzte94taSKS3qdj4dD3+4DAMBp7CZRTU2NK+podifPVEmS2ri3Uligt0Ov8fFord8Ni3JmWQAA2HXRMD937pxuuOEGbdu2TZ6enq6qqVmFBXorc+rg5i4DAACHXfQEuGuuuUbHjx9Xq1aXdZ4cAABwAbspnZaWZruPOQAAaHkaPMz+2WefaeDAgdq4caO++OILLV68WGFhYbX20j/99FOXFAkAABrWYJjffvvtKi8vV2JiYp0z2gEAQMvRYJhfuBb77NmzXVYMAAC4dA1+Zv7jG6oAAICWq8E981OnTikhIeGiL/7p1eEAAIDrNRjm7u7uGjhwoCtrAQAAl6HBMPfw8NCcOXNcWQsAALgMXA0GAACLazDML5zNDgAAWrYGw/zEiROurAMAAFwmDrMDAGBxhDkAABZHmAMAYHGEOQAAFkeYAwBgcYQ5AAAWR5gDAGBxhDkAABZHmAMAYHGEOQAAFkeYAwBgcYQ5AAAWR5gDAGBxhDkAABZHmAMAYHGEOQAAFkeYAwBgcYQ5AAAWR5gDAGBxhDkAABZHmAMAYHGEOQAAFkeYAwBgcYQ5AAAWR5gDAGBxhDkAABZHmAMAYHGEOQAAFkeYAwBgcYQ5AAAWR5gDAGBxhDkAABZHmAMAYHGEOQAAFkeYAwBgcYQ5AAAWR5gDAGBxhDkAABZHmAMAYHEuCfOcnBzFxcUpKipKCQkJKi4urjPm4YcfVvfu3RUTE6PExETt37/fFaUBAGB5LgnzyZMna+bMmcrNzVVycrJmzJhRZ8zw4cO1e/du7dy5U3fddZceffRRV5QGAIDlOT3MS0pKlJeXp+TkZElSWlqa1qxZU2fc8OHD1bp1a0lSv379dPDgQWeXBgDAFcHpYV5UVKTQ0FDbsq+vrzw9PVVaWtrgaxYtWqTbb7/d2aUBAHBFaN3cBfzUK6+8oq+//lqffPJJnefS09OVnp4uScrPz1dWVlaTbfdUZaXtz6ac92pTVlZG/xqJHjYePWw8eth4ruyh08M8JCREhYWFtuWKigqdPn1agYGBdca+/fbbWrx4sT755BN5eXnVeT41NVWpqamSpKSkJMXHxzdZnd5Zm6QTFfL28mrSea82WVlZ9K+R6GHj0cPGo4eN58oeOv0we1BQkCIiIrR27VpJ0tKlS5WSklJn3Nq1a/XUU0/pn//8Z71BDwAA6ueSs9kXLVqkOXPmKDIyUhkZGZo/f74kKTY2VocPH5YkTZw4UZWVlUpKSlJsbKyGDh3qitIAALA8l3xmHh0drW3bttVZn52dbXt87NgxV5QCAMAVhyvAAQBgcYQ5AAAWR5gDAGBxhDkAABZHmAMAYHGEOQAAFkeYAwBgcYQ5AAAWR5gDAGBxhDkAABZHmAMAYHGEOQAAFkeYAwBgcYQ5AAAWR5gDAGBxhDkAABZHmAMAYHGEOQAAFkeYAwBgcYQ5AAAWR5gDAGBxhDkAABZHmAMAYHGEOQAAFkeYAwBgcYQ5AAAWR5gDAGBxhDkAABZHmAMAYHGEOQAAFkeYAwBgcYQ5AAAWR5gDAGBxhDkAABZHmAMAYHGEOQAAFkeYAwBgcYQ5AAAWR5gDAGBxhDkAABZHmAMAYHGEOQAAFkeYAwBgcYQ5AAAWR5gDAGBxhDkAABZHmAMAYHGEOQAAFkeYAwBgcYQ5AAAWR5gDAGBxhDkAABZHmAMAYHGEOQAAFkeYAwBgcYQ5AAAWR5gDAGBxhDkAABZHmAMAYHGEOQAAFkeYAwBgcYQ5AAAW55Iwz8nJUVxcnKKiopSQkKDi4uI6YyoqKnTnnXcqMjJSPXv21GeffeaK0gAAsDyXhPnkyZM1c+ZM5ebmKjk5WTNmzKgz5vnnn1dYWJjy8vL01ltvafz48aqpqXFFeQAAWJrTw7ykpER5eXlKTk6WJKWlpWnNmjV1xr333nt66KGHJEk33nijOnTooG3btjm7PAAALK+1szdQVFSk0NBQ27Kvr688PT1VWlqqwMBA2/rCwkKFhYXZlrt06aLCwkL169fPti49PV3p6emSpPz8fGVlZTVZnacqK21/NuW8V5uysjL610j0sPHoYePRw8ZzZQ+dHuZNKTU1VampqZKkpKQkxcfHN9ncPXK3SkWl6hESqPj4uCab92qTlZXVpP9drkb0sPHoYePRw8ZzZQ+dfpg9JCREhYWFtuWKigqdPn261l65JIWGhurAgQO25YMHD9bao3e2JePjtCDBX0vGE+QAAGtxepgHBQUpIiJCa9eulSQtXbpUKSkpdcbdfffdeu211yRJO3bs0LFjx9S3b19nlwcAgOW55Gz2RYsWac6cOYqMjFRGRobmz58vSYqNjdXhw4clSdOmTdP+/fsVGRmp+++/X8uXL1erVnwNHgAAe1zymXl0dHS9Z6ZnZ2fbHvv5+SkjI8MV5QAAcEVh1xcAAIsjzAEAsDjCHAAAiyPMAQCwOMIcAACLI8wBALA4whwAAIsjzAEAsDjCHAAAiyPMAQCwOMIcAACLI8wBALA4l9xoxRny8/OVlJTUpHMePnxYnTt3btI5rzb0sPHoYePRw8ajh43njB7m5+fXu97NGGOadEsWlpSUpHXr1jV3GZZGDxuPHjYePWw8eth4ruwhh9kBALA496eeeuqp5i6iJendu3dzl2B59LDx6GHj0cPGo4eN56oecpgdAACL4zA7AAAWR5gDAGBxV12Y5+TkKC4uTlFRUUpISFBxcXGdMRUVFbrzzjsVGRmpnj176rPPPmuGSlsuR3r48MMPq3v37oqJiVFiYqL279/fDJW2XI708IIVK1bIzc1Nn3zyiesKtABHe/jiiy/quuuuU69evXTbbbe5uMqWzZEe5ufna/DgwYqNjVV0dLSWLFnSDJW2XA899JCCg4Pl5ubW4Jji4mIlJCQoKipKcXFxysnJafpCzFVm0KBBJiMjwxhjzEsvvWTGjRtXZ8ysWbPMb3/7W2OMMdu3bzfdunUz1dXVLq2zJXOkh//4xz/MuXPnjDHGvPrqq2bEiBEurbGlc6SHxhhTUlJiBgwYYPr37282btzowgpbPkd6+O6775phw4aZU6dOGWOMOXLkiEtrbOkc6eGYMWPMK6+8Yow53z8/Pz9TXl7u0jpbsk2bNpkjR46Yi8XpuHHjzEsvvWSMMSYjI8MMGjSoyeu4qsL8yJEj5tprr7Utnzhxwvj5+dUZ16NHD5OTk2Nbvummm8wXX3zhkhpbOkd7+GPbtm0zvXv3dnZplnEpPfzVr35lNm3aZAYPHkyY/4ijPYyPj+f/3QY42sPU1FTz7LPPGmOM2bdvn+nSpYs5c+aMy+q0iouFuZ+fnzlx4oQxxpiamhrTqVMnU1JS0qTbv6oOsxcVFSk0NNS27OvrK09PT5WWltYaV1hYqLCwMNtyly5dVFhY6LI6WzJHe/hjixYt0u233+6K8izB0R6uXr1a/v7+GjRokKtLbPEc7WFOTo42btyo/v37q3///vrggw9cXWqL5WgP58+fr5UrVyokJEQxMTFatGiR2rRp4+pyLau0tFReXl7y9fWVJLm5uSk0NLTJM8Wyl3OFNbzyyiv6+uuv+bz3Eh0/flxz587Vhg0bmrsUS6uqqtLRo0eVlZWloqIiDRgwQDExMYqIiGju0ixj0aJFeuSRRzRp0iTt3LlTI0aM0DfffCM/P7/mLg0/clXtmYeEhNT6baiiokKnT59WYGBgrXGhoaE6cOCAbfngwYO1foO9mjnaQ0l6++23tXjxYq1bt05eXl6uLLNFc6SHu3fvVlFRkWJjYxUeHq7PP/9co0ePVkZGRnOU3OI4+vewS5cuGj16tG1vKD4+XtnZ2a4ut0VytIcvv/yy7rvvPklSTEyMrr32WuecwHWFCgwMVGVlpU6ePClJMsaosLCwyTPlqgrzoKAgRUREaO3atZKkpUuXKiUlpc64u+++W6+99pokaceOHTp27Jj69u3r0lpbKkd7uHbtWj311FP65z//WW/QX80c6eHNN9+so0ePqqCgQAUFBerfv7/eeeedent9NXL07+Ho0aO1fv16SeePdmzbtk3R0dEurbWlcrSHYWFhyszMlCQdOHBABQUF6tatm0trtbqUlBTbtwA+/PBDRUZGqmPHjk27kSb9BN4Cdu3aZfr06WMiIiLMkCFDzKFDh4wxxsTExNgel5eXm+TkZBMREWGuv/568+mnnzZnyS2OIz3s0KGDCQkJMTExMSYmJsYMGTKkOUtucRzp4Y9xAlxdjvTw5MmTZvTo0SY6Otr07NnTLF26tDlLbnEc6eHnn39u4uLiTO/evU3Pnj3NqlWrmrPkFmf8+PEmODjYSDLBwcHmvvvuM4cOHTIxMTG2MYcOHTJDhgwxkZGRpk+fPmb37t1NXgeXcwUAwOKuqsPsAABciQhzAAAsjjAHAMDiCHMAACyOMAeaQUFBgdzc3LRv377mLuWSpKenq3v37hcds3nzZvn6+qq6utpFVQEgzIFGGDJkiNq0aSNfX1/bT0u4/Kqbm5vtEpKBgYEaPHiw/v3vfzd63tTUVO3du9e2PGHCBNsFRS645ZZbVFFRIXd390Zvrz4XfhHy8fGRr6+vOnTooNtuu01fffXVJc3j5uamf/3rX06pEXA1whxopOnTp6uiosL28+mnnzZ3SZKkv//976qoqFBhYaF69eql4cOHq7y8vLnLajI7d+5URUWF8vPzFRAQoOTk5OYuCWg2hDngBLt27dKtt96qn/3sZ/L399dNN9100eus79y5U4MHD1ZAQIDatWunPn361NoDfuuttxQTEyN/f39FR0frnXfecbgWb29vTZ48WeXl5crLy1N1dbWef/55RUVFyd/fX3379tVHH31kG3/w4EElJSWpffv28vf3V8+ePbV582ZJ0vLlyxUSEiJJevbZZ5Wenq5Vq1bZjkocPHhQn3zyidzc3FRVVaXc3Fy5u7vXujyyJI0aNUqPPvqoJKm6ulovvPCCevToIX9/f/Xp00f/93//5/D78/f31/3336+CggJ99913ks7fP3rEiBEKCgqSn5+fevfurffee8/2mgtXgRs5cqR8fX1tNwJqbC1As2nyy9AAV5HBgwebJ598ss76r7/+2qxfv96cOnXKnD592syePdu0bdvWdtvD/fv3G0kmLy/PGGPMgAEDzB/+8Adz7tw5c+7cOfPll1/a7r29bNkyExoaarZu3Wqqq6vN5s2bjZ+fn9m8eXODdUkymZmZxpjzt7Z8+OGHTUBAgCkvLzcLFiwwwcHBZvv27ebcuXNm5cqV5pprrjHbt283xhgzduxYM3HiRFNZWWmqq6vNnj17zLfffmurJTg42Lad8ePHm9TU1Frb3rhxo5Fku5/9LbfcYmbPnm17/tChQ8bd3d3s3LnTGGPM7NmzTUxMjNmzZ4+prq42q1evNt7e3mbfvn31vref9q60tNTcddddplOnTqaqqsoYY0xhYaH54IMPzIkTJ8zZs2fNkiVLTOvWrc2uXbvq7dEFl1oL0FIQ5kAjDB482Hh4eBh/f3/bz1tvvVXvWH9/f/Phhx8aY+oG0pAhQ0xaWlq9odGrVy+zePHiWusmTpxo0tLSGqxLkvHx8TEBAQGmU6dOJjEx0WzZssUYY0xUVJR56aWXao0fNWqUeeihh4wxxkyYMMGMGDHC7Nq1y9TU1NQadzlh/uabb5ouXbqY6upqY4wxc+fONXFxcbbxbdu2NR9//HGtORITE82cOXPqfW8Xeufn52f8/PyMJNO1a1fzn//8p8F+GGNM7969zZ/+9KdaPfppmF9qLUBLwWF2oJEef/xxlZWV2X7uv/9+HTx4UKNHj1aXLl3Utm1bBQQEqLy8XEePHq13juXLl8vNzU0JCQkKCQnRb3/7W1VUVEiS8vLy9NhjjykgIMD2s3LlSh0+fPiidWVkZOj48eMqLi5WZmam4uPjJUmFhYV1bpQRERGhgwcPSpIWLFigiIgI3XnnnQoKCtKvf/1rlZSUXHZ/7rnnHv3www/KzMyUMUZvvPGGJk6cKEkqKSlReXm57rnnnlrvb8uWLTp06NBF592xY4fKy8u1e/duSdLXX39te+748eN68MEH9fOf/9zW/927dzfY/8bWAjQ37mcOOMGDDz4of39/bd26VUFBQTLGqF27djIN3AohLCxMr7/+uiRp3759Sk5Olo+Pj5555hl16tRJf/jDHzRu3LgmqS00NFT5+fm11uXn56tLly6Szt+yceHChVq4cKEOHTqk++67T1OnTlV6enqduVq1sr8/4OXlpbFjx2rJkiVq3bq1SkpKNGbMGElSQECAPD099Y9//OOyvwVw/fXXa/Hixbrjjjv0y1/+Up07d9aMGTO0Z88ebdq0SaGhoXJzc1NMTEyt/ru5udWapylqAZoLe+aAE/zwww/y9fVVu3btdPLkST3xxBO2Pe36LF++XEVFRTLGqG3btmrdurVatz7/u/Zvf/tbzZkzR1u3blVNTY3OnDmjrVu3avv27ZdV28SJE7VgwQJlZ2erqqpK7777rtatW2fbW37nnXeUn5+vmpoa+fn5ycPDw1bLT3Xq1En5+fl2v1M+ceJEffjhh/rjH/+oX/3qV/Lz85MkeXh4aPLkyZo+fbpycnJkjFFlZaU+/fRT5ebmOvyehg0bpr59+2r27NmSzvff29tbgYGBOnfunP785z/b9uB/XPuPTzJsqlqA5kCYA07wpz/9STt37lS7du10/fXXKzg42HYWeH02btyofv36ydfXVzExMYqPj9fvf/97SdKUKVP01FNPafLkyWrfvr2Cg4M1bdo0nTx58rJqmzp1qn7zm9/o7rvvVvv27fXcc89p9erV6tu3r6TzZ9YnJCTIz89P3bp1U0BAgBYsWFDvXJMmTZIkdejQQQEBAbZD9T914403Kjo6WuvXr7f90nDBggULNGbMGNvh7fDwcM2bN0/nzp27pPc1Z84cLVu2TDk5OZo7d64qKysVFBSk8PBwlZSUaODAgbXGz5s3T88995wCAgI0YsSIJq0FcDVugQoAgMWxZw4AgMUR5gAAWBxhDgCAxRHmAABYHGEOAIDFEeYAAFgcYQ4AgMUR5gAAWNz/A4yh8ZqscpbyAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eB5A5sJ2mTv",
        "outputId": "ef059e2a-ca25-48c5-9a94-4a48f226b055"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'tp': array([0.        , 0.09090909, 0.36363636, 0.36363636, 0.45454545,\n",
              "        0.45454545, 0.54545455, 0.54545455, 0.63636364, 0.63636364,\n",
              "        0.90909091, 0.90909091, 1.        , 1.        ]),\n",
              " 'fp': array([0.        , 0.        , 0.        , 0.03125   , 0.03125   ,\n",
              "        0.04166667, 0.04166667, 0.0625    , 0.0625    , 0.11458333,\n",
              "        0.11458333, 0.1875    , 0.1875    , 1.        ]),\n",
              " 'thresh': array([3.1006000e+00, 2.1006000e+00, 1.4142737e+00, 1.2290132e+00,\n",
              "        1.1956811e+00, 1.1429478e+00, 1.0422109e+00, 8.8665843e-01,\n",
              "        8.5941952e-01, 5.8310997e-01, 5.5342466e-01, 3.2582155e-01,\n",
              "        2.9276037e-01, 9.6741125e-08], dtype=float32),\n",
              " 'auc_score': 0.9393939393939394}"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "roc_to_plot"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Once the ROC curve is plotted, we can find the optimal entropy threshold that discriminates between known and unknown pollen specimens. This can be achieved by minimizing the index of union (IU) developed by Unal (2017)"
      ],
      "metadata": {
        "id": "5U5KLb5bow6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Solve for lowest AUC value\n",
        "\n",
        "# First, define variables used to minimize the IU.  \n",
        "\n",
        "import numpy as np \n",
        "import torch\n",
        "tpr = roc_to_plot['tp']\n",
        "\n",
        "fpr = roc_to_plot['fp']\n",
        "\n",
        "thresh = roc_to_plot['thresh']\n",
        "\n",
        "Spe = 1 - np.array(fpr)\n",
        "Sen = np.array(tpr)\n",
        "\n",
        "auc_score = roc_to_plot['auc_score']"
      ],
      "metadata": {
        "id": "lKVg62Gn2mYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the IU and find the argument that minimizes it\n",
        "\n",
        "IU = torch.abs(torch.tensor((Sen - auc_score))) + torch.abs(torch.tensor((Spe - auc_score)))\n",
        "Diff = torch.abs(torch.tensor(Sen-Spe))\n",
        "IU_minimum_index = np.where(IU == IU.min())\n",
        "Diff_minimum_index = np.where(Diff == Diff.min())\n",
        "\n",
        "if torch.numel(torch.tensor(IU_minimum_index)) == 1:\n",
        "  optimal_threshold = thresh[torch.tensor(IU_minimum_index).item()]\n",
        "else:\n",
        "  optimal_threshold = thresh[torch.tensor(Diff_minimum_index).item()]"
      ],
      "metadata": {
        "id": "pCMTs8bI0Gns",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7ce730c-cd21-4d64-e5c4-dd080ab87a4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute optimal Shannon entropy threshold"
      ],
      "metadata": {
        "id": "aWCmltwDkFz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimal threshold \n",
        "print(optimal_threshold)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7cZ-E05C-YN",
        "outputId": "2217c525-55af-45e4-8ab8-f71242e8aca7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.55342466\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Show optimal threshold on ROC curve"
      ],
      "metadata": {
        "id": "nBQvA5qkkFGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_curve, roc_auc_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def evaluate_openset(scores_closeset, scores_openset):    \n",
        "    y_true = np.array([0] * len(scores_closeset) + [1] * len(scores_openset))\n",
        "    y_discriminator = np.concatenate([scores_closeset, scores_openset])\n",
        "    auc_d, roc_to_plot = plot_roc(y_true, y_discriminator, 'Discriminator ROC')\n",
        "    return auc_d, roc_to_plot\n",
        "\n",
        "\n",
        "def plot_roc(y_true, y_score, title=\"Receiver Operating Characteristic\", **options):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
        "    auc_score = roc_auc_score(y_true, y_score)\n",
        "    roc_to_plot = {'tp':tpr, 'fp':fpr, 'thresh':thresholds, 'auc_score':auc_score}\n",
        "    return auc_score, roc_to_plot\n",
        "\n",
        "plt.figure(figsize=(9,8), dpi=64, facecolor='w', edgecolor='k')\n",
        "ax = plt.gca()\n",
        "plt.plot(fpr, tpr, linewidth=3)\n",
        "plt.plot(fpr[10],tpr[10], 'r*', markersize=15)\n",
        "for tick in ax.xaxis.get_major_ticks():\n",
        "    tick.label.set_fontsize(12) \n",
        "for tick in ax.yaxis.get_major_ticks():\n",
        "    tick.label.set_fontsize(12) \n",
        "plt.grid('on')\n",
        "plt.xlabel('False Positive Rate', fontsize=15)\n",
        "plt.ylabel('True Positive Rate', fontsize=15)\n",
        "plt.title('Novelty Detection Simulation based on the FM ($\\it{P. oleifolius}$)', fontsize=14)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "osEt4eu4dAdA",
        "outputId": "3d28050e-9fe6-4c03-fb53-5004c8332b7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Novelty Detection Simulation based on the FM ($\\\\it{P. oleifolius}$)')"
            ]
          },
          "metadata": {},
          "execution_count": 56
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x512 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1muKkNnHltTzzzrmp1NaCtvqYgQngC9oC",
      "authorship_tag": "ABX9TyOJViL+9qIRo8JTmdStAryy",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}